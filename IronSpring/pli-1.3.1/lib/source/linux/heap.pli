 /* _pli_HeapMgr                                                     */
 /********************************************************************/
 /*                                                                  */
 /*      Module:        _pli_HeapMgr                                 */
 /*      Version        1.0.  Jan, 2010.                             */
 /*      Author:        Peter Flass                                  */
 /*      Function:      Heap Storage Management for Iron Spring PL/I */
 /*                     compiler for Linux.                          */
 /*                                                                  */
 /*      Description:   _pli_HeapMgr is a partial port of            */
 /*                     'ptmalloc', a threaded storage manager       */
 /*                     without lock contention, by Wolfram Gloger   */
 /*                     and Doug Lea.                                */
 /*                                                                  */
 /*      Support for mspaces and mmap are not included.              */
 /*                                                                  */
 /*      Version 1.0 does not support multithreaded applications.    */
 /*                                                                  */
 /* The C original source is available from                          */
 /* Wolfram Gloger: http://www.malloc.de/en/                         */
 /* Copyright (c) 2001-2006 Wolfram Gloger                           */
 /*                                                                  */
 /* Permission to use, copy, modify, distribute, and sell this       */
 /* software and its documentation for any purpose is hereby granted */
 /* without fee, provided that (i) the above copyright notices and   */
 /* this permission notice appear in all copies of the software and  */
 /* related documentation, and (ii) the name of Wolfram Gloger may   */
 /* not be used in any advertising or publicity relating to the      */
 /* software.                                                        */
 /*                                                                  */
 /*      Support for mspaces and mmap are not included.              */
 /*                                                                  */
 /*      Notes:                                                      */
 /*      . This port would have been much easier had the PL/I        */
 /*        preprocessor been available to replicate the C macros.    */
 /*                                                                  */
 /*      . Handling of bit strings is considerably different in PL/I */
 /*        than C.  This code attempts to preserve the C definition  */
 /*        at the expense of efficiency.  This should be changed late*/
 /*                                                                  */
 /*      Entry points:                                               */
 /*        _pli_alloc(bytes): Allocates 'bytes' bytes of storage     */
 /*                           and returns the address to the caller. */
 /*        _pli_free(mem):    Frees previously allocated block of    */
 /*                           storage at address 'mem.               */
 /*                                                                  */
 /*      To Do: This code is particularly opaque (it's C, after all) */
 /*             It needs much better diagnostic facilities.          */
 /*                                                                  */
 /*      Modifications:                                              */
 /*          2023-09-20: Move "heap storage error 5" before     1.2.0*/
 /*                      "erroraction"                          1.2.0*/
 /*          2021-05-17: Raise STORAGE condition on errors    0.9.10e*/
 /*                      to aid debugging                     0.9.10e*/
 /*          2018-03-03: Attempt to prevent recursive errors  0.9.10b*/
 /*          2011-05-15: Add mutex support for multitasking     0.9.2*/
 /*                                                                  */
 /********************************************************************/
heap_mgr: proc ext( '_pli_HeapMgr' );
 
 /*--------------------------*/
 /* Parameters               */
 /*--------------------------*/
 dcl      bytes               fixed bin(31);
 dcl      mem                 ptr;
 
 /*--------------------------*/
 /* Compile-time constants   */
 /*--------------------------*/
 %replace NSMALLBINS        by  32;
 %replace NTREEBINS         by  32;
 %replace SMALLBIN_SHIFT    by   3;
 %replace SMALLBIN_WIDTH    by   8;  /* (SIZE_T_ONE << SMALLBIN_SHIFT) */
 %replace TREEBIN_SHIFT     by   8;
 %replace MIN_LARGE_SIZE    by 256;  /* (SIZE_T_ONE << TREEBIN_SHIFT)  */
 %replace MAX_SMALL_SIZE    by 255;  /* (MIN_LARGE_SIZE - SIZE_T_ONE)  */
 %replace MALLOC_ALIGNMENT  by   8;
 %replace CHUNK_ALIGN_MASK  by   7;  /* (MALLOC_ALIGNMENT - SIZE_T_ONE) */
 %replace CHUNK_OVERHEAD    by   8;
 %replace MAX_SMALL_REQUEST by 240;  /* (MAX_SMALL_SIZE - CHUNK_ALIGN_MASK - CHUNK_OVERHEAD) */
 %replace M_TRIM_THRESHOLD  by  -1;
 %replace M_GRANULARITY     by  -2;
 %replace M_MMAP_THRESHOLD  by  -3;
 /* MCHUNK_SIZE = sizeof(mchunk) = 16 */
 %replace MIN_CHUNK_SIZE    by   16;   /* (MCHUNK_SIZE + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK */
 %replace MAX_REQUEST       by   1073741823;     /*  ((-MIN_CHUNK_SIZE) << 2) */ 
 %replace MIN_REQUEST       by   7;    /* (MIN_CHUNK_SIZE - CHUNK_OVERHEAD - SIZE_T_ONE) */
 %replace PINUSE_BIT        by   1;      
 %replace CINUSE_BIT        by   2;
 %replace FLAG4_BIT         by   4;
 %replace INUSE_BITS        by   3;              /* (PINUSE_BIT|CINUSE_BIT) */
 %replace FLAG_BITS         by   7;              /* (PINUSE_BIT|CINUSE_BIT|FLAG4_BIT) */
 %replace FENCEPOST_HEAD    by   7;              /* (INUSE_BITS|SIZE_T_SIZE) */
 %replace DEFAULT_GRANULARITY    by       4096;
 %replace MAX_SIZE_T             by 2147483647;
 %replace HALF_MAX_SIZE_T        by 1073741823;
 %replace DEFAULT_MMAP_THRESHOLD by 1073741823;  /* disables mmap */
 %replace MAX_RELEASE_CHECK_RATE by 1073741823;
 %replace DEFAULT_TRIM_THRESHOLD by    2097152;  /* 2MB */
 /* values for mflags */
 %replace USE_NONCONTIGUOUS_BIT  by 4; 
 %replace USE_LOCK_BIT           by 2;
 
 %include errno;

 /*--------------------------*/
 /* Static Data              */
 /*--------------------------*/
 dcl      recursive_error     bit(1)    static    init( '0'b );/*.10b*/
 /* NOTE: A size_t should be an unsigned integer, hence MAX_SIZE_T   */
 /*       is all ones.  Since our equivalent is currently a signed   */
 /*       value 'max_size_t' should be bin('7FFFFFFF'bx).            */
 /*       hopefully, this will make little practical difference.     */
 dcl      max_size_t_con          fixed bin(31)       static    
             init( MAX_SIZE_T );

 dcl      magic_init_mutex (3)fixed bin(31)       static   init(0,0,0);

 dcl      morecore_mutex   (3)fixed bin(31)       static   init(0,0,0);

 /* NOTE: This is a macro in the original, but here is a static    */
 /*       variable initialized once at runtime.                    */
 dcl      top_foot_size       fixed bin(31)       static;

 dcl    1 malloc_state        static,   
          5 smallmap          fixed bin(31), /* binmap_t  */
	  5 treemap           fixed bin(31), /* ditto     */
	  5 dvsize            fixed bin(31), /* size_t    */
	  5 topsize           fixed bin(31), /* ditto     */
	  5 least_addr        ptr,           /* char *    */
	  5 dv                ptr,           /* mchunkptr */
	  5 top               ptr,           /* ditto     */
	  5 trim_check        fixed bin(31), /* size_t    */
	  5 release_checks    fixed bin(31), /* ditto     */
	  5 magic             fixed bin(31), /* ditto     */
	  5 smallbins   (0:65)ptr,           /* ((NSMALLBINS+1)*2)mchunkptr */
	  5 treebins    (0:31)ptr,           /* (NTREEBINS)tbinptr        */
	  5 footprint         fixed bin(31), /* size_t    */
	  5 max_footprint     fixed bin(31), /* ditto     */
	  5 mflags            fixed bin(31), /* flag_t    */
            /* 0000 0001 ... - USE_MMAP_BIT          (1U) */
            /* 0000 0010 ... - USE_LOCK_BIT          (2U) */
            /* 0000 0100 ... - USE_NONCONTIGUOUS_BIT (4U) */
            /* 0000 1000 ... - EXTERN_BIT            (8U) */
	  5 mutex             like mlock_t,
	  5 seg               like malloc_segment,
	  5 extp              ptr,           /* void*     */
	  5 exts              fixed bin(31); /* size_t    */
	 
 dcl    1 mparams            static,  
          5 magic            fixed bin(31)   init(0), /* size_t */
	  5 page_size        fixed bin(31), /* ditto  */
	  5 granularity      fixed bin(31), /* ditto  */
	  5 mmap_threshold   fixed bin(31), /* ditto  */
	  5 trim_threshold   fixed bin(31), /* ditto  */
	  5 default_mflags   fixed bin(31)   init(6); /* flag_t */
	    /* USE_NONCONTIGUOUS|USE_LOCK             */

 
 /*--------------------------*/
 /* Prototypes               */
 /*--------------------------*/
 dcl    CMFAIL                ptr       based( addr(max_size_t_con) );
 dcl    aPtr                  ptr based; 
 dcl    1 mlock_t             based,
	  5 l                 fixed bin(31), /* unsigned int */
	  5 c                 fixed bin(31); /* unsigned int */
	  
 dcl    1 malloc_segment      based,
	  5 base              ptr, 	     /* char * */
	  5 size              fixed bin(31), /* size_t */
	  5 next              ptr,	     /* struct malloc_segment* */
	  5 sflags            fixed bin(31); /* flag_t */
		  
	      
 dcl    1 malloc_chunk        based,
          5 prev_foot         fixed bin(31), /* size_t  */
	  /* Since every chunk is allocated on an eight-byte boundary */
	  /* the low-order three bits of 'head' are used for flags.   */
          /* This is definitely *non-portable*.                       */
          5 head              fixed bin(31), /* size_t  */
          5 fd                ptr,           /* malloc_chunk*  */
          5 bk                ptr;           /* malloc_chunk*  */
	      
 dcl    1 malloc_tree_chunk    based,
          /* The first four fields must be compatible with malloc_chunk */
          5 prev_foot         fixed bin(31), /* size_t  */
	  /* Since every chunk is allocated on an eight-byte boundary */
	  /* the low-order three bits of 'head' are used for flags.   */
          /* This is definitely *non-portable*.                       */
          5 head              fixed bin(31), /* size_t  */
          5 fd                ptr,           /* malloc_chunk*  */
          5 bk                ptr,           /* malloc_chunk*  */
          5 child        (0:1)ptr,           /* struct malloc_tree_chunk* */
          5 parent            ptr,           /* struct malloc_tree_chunk* */
          5 index             fixed bin(31); /* bindex_t       */

 /* Direct Linux syscalls  */
 %replace SYS_TIME  by 13;
 %replace SYS_BRK   by 45;
 dcl     syscall	     entry
  		             returns( fixed bin(31) )
		             options( linkage(system) )
		             ext( '_pli_Syscall' );
 dcl     mutex_init          entry(ptr,fixed bin(31))         /*0.9.2*/
                             options( linkage(system) )       /*0.9.2*/
                             ext( '_pli_mutex_init' );        /*0.9.2*/
 dcl	 mutex_wait	     entry(ptr)                       /*0.9.2*/
                             returns( fixed bin(31) )         /*0.9.2*/			     
			     options( LINKAGE(SYSTEM) )       /*0.9.2*/
			     external( '_pli_mutex_wait' );   /*0.9.2*/
 dcl	 mutex_post	     entry(ptr)			      /*0.9.2*/
                             returns( fixed bin(31) )         /*0.9.2*/			     
			     options( LINKAGE(SYSTEM) )       /*0.9.2*/
			     external( '_pli_mutex_post' );   /*0.9.2*/
 dcl     osexit              entry                          /*0.9.10b*/
                             options( linkage(system) )     /*0.9.10b*/
                             ext( '_pli_Exit' );            /*0.9.10b*/
 dcl    (addr,
         binvalue,
 	 bool,
	 barrier,                                             /*0.9.2*/
	 floor,
	 length,
	 null,
	 ptradd,
	 ptrvalue,
         stg,
	 string,
	 substr,
	 sysnull 
	)                     builtin;
           	 
%page;	             
/*-----------------------------------------------*/
/* Externally-callable entry-points              */
/*-----------------------------------------------*/
malloc: entry(bytes) returns( ptr )
        ext( '_pli_alloc' );
  return( internal_dlmalloc(bytes) );        
  
free:   entry(mem)
        ext( '_pli_free' );
  call internal_dlfree(mem);
  return;

%page;  
/*-----------------------------------------------*/
/* dlmalloc                                      */
/*-----------------------------------------------*/
internal_dlmalloc: proc(bytes) returns( ptr );
  dcl     bytes               fixed bin(31);
  dcl     mem                 ptr;
  dcl     nb                  fixed bin(31);
  dcl     idx                 fixed bin(31);      /* bindex_t */
  dcl     smallbits           fixed bin(31);      /* binmap_t */
  dcl    (b, p, r)            ptr;                /* mchunkptr */
  dcl     rsize               fixed bin(31);      /* size_t */
  dcl     i                   fixed bin(31);      /* bindex_t */
  dcl     leftbits            fixed bin(31);      /* binmap_t */
  dcl     leastbit            fixed bin(31);      /* binmap_t */
  dcl     dvs                 fixed bin(31);      /* size_t */
  dcl     F                   ptr;                /* mchunkptr */
  dcl     left_bits           fixed bin(31);

  /*
     Basic algorithm:
     If a small request (< 256 bytes minus per-chunk overhead):
       1. If one exists, use a remainderless chunk in associated smallbin.
          (Remainderless means that there are too few excess bytes to
          represent as a chunk.)
       2. If it is big enough, use the dv chunk, which is normally the
          chunk adjacent to the one used for the most recent small request.
       3. If one exists, split the smallest available chunk in a bin,
          saving remainder in dv.
       4. If it is big enough, use the top chunk.
       5. If available, get memory from system and use it
     Otherwise, for a large request:
       1. Find the smallest available binned chunk that fits, and use it
          if it is better fitting than dv chunk, splitting if necessary.
       2. If better fitting than any binned chunk, use the dv chunk.
       3. If it is big enough, use the top chunk.
       4. If request size >= mmap threshold, try to directly mmap this chunk.
       5. If available, get memory from system and use it

     The ugly goto's here ensure that postaction occurs along all paths.
  */
  
  if ^preaction() then do;

    /* Allocate a small chunk        */
    if bytes <= MAX_SMALL_REQUEST then do;
      /* nb = (bytes < MIN_REQUEST)? MIN_CHUNK_SIZE : pad_request(bytes) */
      /* #define pad_request(req)    */
      /*   (((req) + CHUNK_OVERHEAD + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK) */

      if bytes<MIN_REQUEST then nb = MIN_CHUNK_SIZE;
      else nb =
        IAND(bytes+CHUNK_OVERHEAD+CHUNK_ALIGN_MASK,INOT(CHUNK_ALIGN_MASK)); /* pad_request(bytes); */
      idx = ISRL(nb,SMALLBIN_SHIFT);                 /* idx = small_index(nb)  */
      smallbits = ISRL(malloc_state.smallmap,idx);   /* smallbits = gm->smallmap >> idx */
      /* Remainderless fit to a smallbin. */
      if IAND(smallbits,3)^=0 then do;     /* ((smallbits & 0x3U) != 0) */
        idx = idx + IAND(INOT(smallbits),1); /* Uses next bin if idx empty */
        /* #define smallbin_at(M, i) ((sbinptr)((char*)&((M)->smallbins[(i)<<1]))) */
        b = addr(malloc_state.smallbins(ISLL(idx,1)));   /* smallbin_at(gm, idx) */
        p = b->malloc_chunk.fd;
        /* call unlink_first_small_chunk(gm, b, p, idx) */
        F = p->malloc_chunk.fd; 
        if b = F                          /* This is the first   */
        /* clear_smallmap(gm, i) */
        then do;
	  malloc_state.smallmap = IAND(malloc_state.smallmap,INOT(ISLL(1,idx))); 
	  end;
        else if F>=malloc_state.least_addr then do; /* ok_address(gm, F) */
          b->malloc_chunk.fd = f; 
          F->malloc_chunk.bk = b; 
          end; /* else */
        else do;                                              /*0.9.2*/
          call release_lock( addr(malloc_state.mutex) );      /*0.9.2*/
          if recursive_error then call osexit;              /*0.9.10b*/
	  display( 'Heap storage error 1' );                  /*0.9.2*/
          recursive_error = '1'b;                           /*0.9.10b*/
	  signal ERROR; 
	  end;                                                /*0.9.2*/
        /* set_inuse_and_pinuse(gm, p, small_index2size(idx)) */
        /* #define set_inuse_and_pinuse(M,p,s)\                       */
        /*     ((p)->head = (s|PINUSE_BIT|CINUSE_BIT),\               */
        /*     ((mchunkptr)(((char*)(p)) + (s)))->head |= PINUSE_BIT) */
        p->malloc_chunk.head = IOR( ISLL(idx,3), INUSE_BITS );
        ptradd(p,ISLL(idx,3))->malloc_chunk.head = 
	       IOR( ptradd(p,ISLL(idx,3))->malloc_chunk.head, PINUSE_BIT );
        /* #define chunk2mem(p) ((void*)((char*)(p) + TWO_SIZE_T_SIZES)) */
        mem = p+8;                          /* chunk2mem(p) */
        goto postaction;
        end; /* smallbits */

      else if nb > malloc_state.dvsize then do;
        if smallbits ^= 0 then do; /* Use chunk in next nonempty smallbin */
          /* leftbits = (smallbits << idx) & left_bits(idx2bit(idx)); */
          /* #define left_bits(x) ((x<<1) | -(x<<1)) */
          left_bits = ISLL(1,idx);        /* idx2bit(idx) */
          left_bits = IOR( ISLL(left_bits,1), -(ISLL(left_bits,1)) ); /* left_bits(idx2bit(idx)) */
          leftbits  = IAND(ISLL(smallbits,idx),left_bits);
          /* 'least_bit' returns the lowest bit set                   */
          leastbit = IAND(leftbits,-leftbits); /* #define least_bit(x) ((x) & -(x)) */
          call BitScanForward(i,leastbit); /* compute_bit2idx(leastbit, i) */
          /* #define smallbin_at(M, i) ((sbinptr)((char*)&((M)->smallbins[(i)<<1]))) */
          b = addr(malloc_state.smallbins(ISLL(i,1)));/* smallbin_at(gm, i) */
          p = b->malloc_chunk.fd;
          /* unlink_first_small_chunk(gm, b, p, i); */
          F = p->malloc_chunk.fd; 
          if b = F                          /* This is the first   */
          /* clear_smallmap(gm, i) */
          then do;
	    malloc_state.smallmap = IAND(malloc_state.smallmap,INOT(ISLL(1,i))); 
	    end;
          else if F>=malloc_state.least_addr then do; /* ok_address(gm, F) */
            b->malloc_chunk.fd = f; 
            F->malloc_chunk.bk = b; 
            end; /* else */
          else do;                                            /*0.9.2*/
	    display( 'Heap storage error 2' );                /*0.9.2*/
            call release_lock( addr(malloc_state.mutex) );    /*0.9.2*/
	    signal ERROR; 
	    end;                                              /*0.9.2*/
	  rsize = ISLL(i,3)-nb;             /* small_index2size(i) - nb */
          /* set_size_and_pinuse_of_inuse_chunk(gm, p, nb) */
          /* #define set_size_and_pinuse_of_inuse_chunk(M, p, s)\ */
          /*    ((p)->head = (s|PINUSE_BIT|CINUSE_BIT))           */
          p->malloc_chunk.head = IOR(nb,INUSE_BITS);
          r = p+nb;                         /* chunk_plus_offset(p, nb) */
          /* set_size_and_pinuse_of_free_chunk(r, rsize) */
	  /* #define set_size_and_pinuse_of_free_chunk(p, s)\ */
          /*   ((p)->head = (s|PINUSE_BIT), set_foot(p, s))   */
          /* #define set_foot(p, s)  (((mchunkptr)((char*)(p) + (s)))->prev_foot = (s)) */
          r->malloc_chunk.head = IOR(rsize,PINUSE_BIT); /* set_size_and_pinuse_of_free_chunk(r, rsize) */
          ptradd(r,rsize)->malloc_chunk.prev_foot = rsize;
          call replace_dv(r, rsize);
          mem = p+8;                        /* chunk2mem(p) */
          /* check_malloced_chunk(gm, mem, nb);   DEBUG only*/
          goto postaction;
          end; /* smallbits^=0 */
        else if malloc_state.treemap ^= 0 then do;
            mem = tmalloc_small(nb);
            if mem ^= sysnull() then do;
              /* check_malloced_chunk(gm, mem, nb);   DEBUG only */
              goto postaction;
              end; /* mem^=sysnull */
            end; /* treemap */
        end; /* nb>gm->dvsize */
      end; /* bytes<=MAX_SMALL_REQUEST */
      
    /* Request too large             */
    else if bytes >= MAX_REQUEST
    then nb = MAX_SIZE_T; /* Too big to allocate. Force failure (in sys alloc) */
      
    /* Allocate a large chunk        */
    else do;
      /* #define pad_request(req) */
      /*   (((req) + CHUNK_OVERHEAD + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK) */
      /* nb = pad_request(bytes); */
      nb = IAND(bytes+CHUNK_OVERHEAD+CHUNK_ALIGN_MASK,INOT(CHUNK_ALIGN_MASK));
      if malloc_state.treemap ^= 0 & comp1(mem,tmalloc_large(nb)) ^= 0 
      then do;
        /* check_malloced_chunk(gm, mem, nb);   DEBUG only */
        goto postaction;
        end;
      end; /* else */

    if nb <= malloc_state.dvsize then do;
      rsize = malloc_state.dvsize - nb;
      p = malloc_state.dv;
      if rsize >= MIN_CHUNK_SIZE then do; /* split dv */
        r,malloc_state.dv = p+nb;                  /* chunk_plus_offset(p, nb) */
        malloc_state.dvsize = rsize;
        /* set_size_and_pinuse_of_free_chunk(r, rsize); */
        r->malloc_chunk.head = IOR(rsize,PINUSE_BIT);
        ptradd(r,rsize)->malloc_chunk.prev_foot = rsize;
        /* set_size_and_pinuse_of_inuse_chunk(gm, p, nb); */
        p->malloc_chunk.head = IOR(nb,INUSE_BITS);
        end; /* rsize */
      else do; /* exhaust dv */
        dvs = malloc_state.dvsize;
        malloc_state.dvsize = 0;
        malloc_state.dv = sysnull();
        /* set_inuse_and_pinuse(gm, p, dvs) */
        p->malloc_chunk.head = IOR( dvs, INUSE_BITS );
	ptradd(p,dvs)->malloc_chunk.head = 
	  IOR(ptradd(p,dvs)->malloc_chunk.head,PINUSE_BIT);
        end; /* else */
      mem = p+8;              /* chunk2mem(p) */
      /* check_malloced_chunk(gm, mem, nb);   DEBUG only */
      goto postaction;
      end; /* nb<=gm->dvsize */

    else if nb < malloc_state.topsize then do; /* Split top */
      rsize,malloc_state.topsize = malloc_state.topsize - nb;
      p = malloc_state.top;
      r,malloc_state.top = p+nb;               /* chunk_plus_offset(p, nb) */
      /* r->head = rsize | PINUSE_BIT; */
      r->malloc_chunk.head = IOR(rsize,PINUSE_BIT);
      /* set_size_and_pinuse_of_inuse_chunk(gm, p, nb); */
      p->malloc_chunk.head = IOR(nb,INUSE_BITS);
      mem = p+8;              /* chunk2mem(p) */
      goto postaction;
      end; /* nb<gm->topsize */

    mem = sys_alloc(nb);

  postaction:
    call release_lock( addr(malloc_state.mutex) );
    return( mem );
    end; /* ^preaction */

  return( sysnull() );

/* replace c's assignment/comparison combo */
/* assign p2 to p1, return 0 if p1 is sysnull */
comp1: proc(p1,p2) returns( fixed bin(31) );
  dcl   (p1,p2)              ptr;
  p1 = p2;
  return( binaryvalue(p1) );
  end comp1;
  

/* allocate a large request from the best fitting chunk in a treebin */
tmalloc_large: proc(nb) returns( ptr );
  dcl    nb                  fixed bin(31);               /* size_t */
  dcl    v                   ptr       init( sysnull() ); /* tchunkptr */
  dcl    t                   ptr;                         /* tchunkptr */
  dcl    rst                 ptr;                         /* tchunkptr */
  dcl    rt                  ptr;                         /* tchunkptr */
  dcl    rsize               fixed bin(31);               /* size_t    */  
  dcl    sizebits            fixed bin(31);               /* size_t    */  
  dcl    trem                fixed bin(31);               /* size_t    */  
  dcl    idx                 fixed bin(31);               /* bindex_t  */
  dcl    leftbits            fixed bin(31);               /* binmap_t  */
  dcl    i                   fixed bin(31);               /* bindex_t */
  dcl    leastbit            fixed bin(31);               /* binmap_t */
  dcl    r                   ptr;                         /* mchunkptr */
  dcl    shft                fixed bin(31);               /* PL/I */

  /*rsize = -nb; /* Unsigned negation */
  rsize = max_size_t;
  idx = compute_tree_index(nb,idx);     /* compute_tree_index(nb, idx) */
  if idx=NTREEBINS-1 then shft = 0;
  else                    shft = 31-(ISRL(idx,1)+6);
  /* #define treebin_at(M,i)     (&((M)->treebins[i])) */
  t = malloc_state.treebins(idx);      /* if ((t = *treebin_at(m, idx)) != 0) */
  if t^=sysnull() then do;
    /* Traverse tree for this bin looking for node with size == nb */
    /* sizebits = nb << leftshift_for_tree_index(idx); */
    /* #define leftshift_for_tree_index(i) \ */
    /*    ((i == NTREEBINS-1)? 0 : \         */
    /* ((SIZE_T_BITSIZE-SIZE_T_ONE) - (((i) >> 1) + TREEBIN_SHIFT - 2))) */
    sizebits = ISLL(nb,shft);
    rst = sysnull();  /* The deepest untaken right subtree */
 for:do while( '1'b );              /* for (;;) */
      trem = chunksize(t) - nb;
      if trem < rsize & trem > 0 then do;
        v = t;
        rsize=trem;                 /* if (rsize = trem) == 0) break */
        if rsize=0 then leave for;
        end; /* trem */
      rt = t->child(1);
      /* t = t->child[(sizebits >> (SIZE_T_BITSIZE-SIZE_T_ONE)) & 1]; */
      t = t->child(IAND(ISRL(sizebits,31),1));
      if rt ^= sysnull() & rt ^= t
      then rst = rt;
      if t = sysnull() then do;    /* No better fit */
        t = rst; /* set t to least subtree holding sizes > nb */
        leave for;                 /* break; */
        end; /* t */
      sizebits = ISLL(sizebits,1); /* sizebits <<= 1 */
      end for;
    end; /* if */

  if t = sysnull() & v = sysnull() then do; /* set t to root of next non-empty treebin */
    /* leftbits = left_bits(idx2bit(idx)) & m->treemp */
    /* #define left_bits(x) ((x<<1) | -(x<<1)) */
    /* #define idx2bit(i) ((binmap_t)(1) << (i)) */
    leftbits = IAND(IOR(ISLL(1,idx),-(ISLL(1,idx))),malloc_state.treemap); /* idx2bit(idx) */
    if leftbits ^= 0 then do;
      leastbit = IAND(leftbits,-leftbits); /* #define least_bit(x) ((x) & -(x)) */
      call BitScanForward(i,leastbit); /* compute_bit2idx(leastbit, i) */
      /* #define treebin_at(M,i)     (&((M)->treebins[i])) */
      t = malloc_state.treebins(i);   /* *treebin_at(m, i) */
      end; /* leftbits */
    end; /* t */

  do while (t ^= sysnull());    /* find smallest of tree or subtree */
    trem = chunksize(t) - nb;
    if trem < rsize & trem>0 then do;
      rsize = trem;
      v = t;
      end; /* trem */
    /* #define leftmost_child(t) ((t)->child[0] != 0? (t)->child[0] : (t)->child[1]) */
    if t->child(0)^=sysnull()   /* t = leftmost_child(t) */
    then t = t->child(0);
    else t = t->child(1);
    end; /* do */  

  /*  If dv is a better fit, return 0 so malloc will use it */
  if v ^= sysnull() & rsize < (malloc_state.dvsize - nb) &
     rsize>0 &
     malloc_state.dvsize-nb>0 then do;
    /* if (RTCHECK(ok_address(m, v))) then do /* split */
    r = v+nb;                        /* chunk_plus_offset(v, nb) */
    /* if (RTCHECK(ok_next(v, r))) then do */
    call unlink_large_chunk(v);
    if (rsize < MIN_CHUNK_SIZE) then do;
      /* set_inuse_and_pinuse(m, v, (rsize + nb)) */
      v->malloc_chunk.head = IOR( rsize + nb, INUSE_BITS );
      ptradd(v,(rsize+nb))->malloc_chunk.head =
         IOR(ptradd(v,(rsize+nb))->malloc_chunk.head,PINUSE_BIT);
      end;
    else do;
      /* set_size_and_pinuse_of_inuse_chunk(m, v, nb); */
      v->malloc_chunk.head = IOR(nb,INUSE_BITS);
      /* set_size_and_pinuse_of_free_chunk(r, rsize); */
      r->malloc_chunk.head = IOR(rsize,PINUSE_BIT);
      ptradd(r,rsize)->malloc_chunk.prev_foot = rsize;
      /* insert_chunk(m, r, rsize) */
      /* #define insert_chunk(M, P, S)\ */
      /*   if (is_small(S)) insert_small_chunk(M, P, S)\ */
      /*   else { tchunkptr TP = (tchunkptr)(P); insert_large_chunk(M, TP, S); } */
      if ISRL(rsize,SMALLBIN_SHIFT) < NSMALLBINS   /* if is_small(rsize) */
      then call insert_small_chunk(r,rsize);
      else call insert_large_chunk(r,rsize);
      end; /* else */
    return( v+8 /*chunk2mem(v)*/ );
    end; /* v */
  return( sysnull() );
  end tmalloc_large;

/* allocate a small request from the best fitting chunk in a treebin */
tmalloc_small: proc(nb) returns( ptr );
  dcl    nb                  fixed bin(31); /* size_t */
  dcl   (t,v)                ptr;           /* tchunkptr */
  dcl    rsize               fixed bin(31); /* size_t */
  dcl    i                   fixed bin(31); /* bindex_t */
  dcl    leastbit            fixed bin(31); /* binmap_t */
  dcl    trem                fixed bin(31); /* size_t */
  dcl    r                   ptr;           /* mchunkptr */
  dcl    l                   ptr;           /* PL/I */

  /* #define least_bit(x) ((x) & -(x)) */  
  leastbit = IAND(malloc_state.treemap,-malloc_state.treemap);
  call BitScanForward(i,leastbit);  /* compute_bit2idx(leastbit, i) */
  /* #define treebin_at(M,i)     (&((M)->malloc_state.treebins[i])) */
  v,t = malloc_state.treebins(i); /* *treebin_at(m, i) */
  rsize = chunksize(t) - nb;
  /* while ((t = leftmost_child(t)) != 0) { */
  /* #define leftmost_child(t) ((t)->child[0] != 0? (t)->child[0] : (t)->child[1]) */
  if t->child(0) ^= sysnull       /* t=leftmost_child(t) */
  then l = t->child(0);
  else l = t->child(1);
  do while(l^=sysnull());
    t = l;
    trem = chunksize(t) - nb;
    if trem < rsize & rsize>0 then do;
      rsize = trem;
      v = t;
      end; /* trem */
    if t->child(0) ^= sysnull
    then l = t->child(0);
    else l = t->child(1);
    end; /* do while */

  r = v+nb;               /* chunk_plus_offset(v, nb) */
  call unlink_large_chunk(v);
  if rsize < MIN_CHUNK_SIZE & rsize>0 then do;
     /* set_inuse_and_pinuse(m, v, (rsize + nb)) */
     v->malloc_chunk.head = IOR( rsize + nb, INUSE_BITS );
     ptradd(v,(rsize+nb))->malloc_chunk.head = 
        IOR(ptradd(v,(rsize+nb))->malloc_chunk.head, PINUSE_BIT );
     end;
  else do;
    /* set_size_and_pinuse_of_inuse_chunk(m, v, nb); */
    v->malloc_chunk.head = IOR(nb,INUSE_BITS);
    /* set_size_and_pinuse_of_free_chunk(r, rsize); */
    r->malloc_chunk.head = IOR(rsize,PINUSE_BIT);
    ptradd(r,rsize)->malloc_chunk.prev_foot = rsize;
    call replace_dv(r, rsize);
    end; /* else */
  return(v+8); /*chunk2mem(v)*/
  end tmalloc_small;

  end internal_dlmalloc;

%page;
/*-----------------------------------------------*/
/* dlfree                                        */
/*-----------------------------------------------*/
internal_dlfree: proc(mem);
  dcl     mem                 ptr;
  dcl     p                   ptr;           /* mchunkptr */
  dcl     psize               fixed bin(31); /* size_t */
  dcl     tsize               fixed bin(31); /* size_t */
  dcl     dsize               fixed bin(31); /* size_t */
  dcl     nsize               fixed bin(31); /* size_t */
  dcl     prev                ptr;           /* mchunkptr */
  dcl     next                ptr;           /* mchunkptr */
  dcl     prevsize            fixed bin(31); /* size_t */
  dcl     tp                  ptr;           /*tchunkptr */

  /*
     Consolidate freed chunks with preceeding or succeeding bordering
     free chunks, if they exist, and then place in a bin.  Intermixed
     with special cases for top, dv, mmapped chunks, and usage errors.
  */
	  
  if mem ^= sysnull() then do;
    p  = mem-8;                  /* p  = mem2chunk(mem) */
    if malloc_state.magic^=mparams.magic then do;
      display( 'Heap storage error 3' );                  /*0.9.2*/
      call release_lock( addr(malloc_state.mutex) );      /*0.9.3*/
      signal ERROR;  /* USAGE_ERROR_ACTION(gm, p) */
      end;     
   
    if ^preaction() then do;
      /* check_inuse_chunk(fm, p) */
      /* if p>=gm->least_addr & ok_cinuse(p) then do */
      /* if (RTCHECK(ok_address(fm, p) && ok_cinuse(p))) */
      if p>=malloc_state.least_addr & 
         IAND(p->malloc_chunk.head,CINUSE_BIT)^=0 then do;
        psize = chunksize(p);
        next  = p+psize;                           /* chunk_plus_offset(p, psize) */

        /********************************************************/
        /* See if this chunk can be consolidated with the       */
	/* previous one                                         */
        /********************************************************/
        /* if (!pinuse(p)) { */
	/* Is the previous chunk free? */
        if IAND(p->malloc_chunk.head,PINUSE_BIT)=0 then do;
          prevsize = p->malloc_chunk.prev_foot;
	  /* prev = chunk_minus_offset(p, prevsize) */
          prev = p-prevsize;            /* chunk_minus_offset(p, prevsize)*/
          psize = psize + prevsize;     /* combined size        */    
          p = prev;                     /* addr(combined chunk) */
	  /* if ( ok_address(fm, prev) ) */ 
          if prev>=malloc_state.least_addr then do;/* consolidate backward */
	    /* Unlink combined chunk     */
            if p ^= malloc_state.dv then do;
              /* unlink_chunk(fm, p, prevsize); */
              /* #define unlink_chunk(M, P, S)\ */
              /*   if (is_small(S)) unlink_small_chunk(M, P, S)\ */
              /*   else { tchunkptr TP = (tchunkptr)(P); unlink_large_chunk(M, TP); } */
              /* if is_small(prevsize)  */
              if ISRL(prevsize,SMALLBIN_SHIFT) < NSMALLBINS
              then call unlink_small_chunk(p,prevsize);
              else call unlink_large_chunk(p);
              end; /* p^=gm->dv */
            /* else if ((next->head & INUSE_BITS) == INUSE_BITS) { */
            else if IAND(next->malloc_chunk.head,INUSE_BITS) = INUSE_BITS 
	    then do;
	      /* Else if this is the designated victim, turn off  */
	      /* the 'pinuse' bit on the next chunk and update    */
	      /* the dv info in malloc_state'                     */
              malloc_state.dvsize = psize;
              /* set_free_with_pinuse(p, psize, next); */
              /* #define set_free_with_pinuse(p, psize, next) */
              /*   (clear_pinuse(next), set_size_and_pinuse_of_free_chunk(p, psize)) */
              /* #define clear_pinuse(next) ((next)->head &= ~PINUSE_BIT) */
              next->malloc_chunk.head = IAND(next->malloc_chunk.head,INOT(PINUSE_BIT));  /* clear_pinuse(next) */
              /* #define set_size_and_pinuse_of_free_chunk(p, psize) */
              /*   ((p)->head = (psize|PINUSE_BIT), set_foot(p, psize))  */
              p->malloc_chunk.head = IOR(psize,PINUSE_BIT);        /* set_size_and_pinuse_of_free_chunk(p, s) */
              /* #define set_foot(p, psize)  (((mchunkptr)((char*)(p) + (psize)))->prev_foot = (psize)) */
              ptradd(p,psize)->malloc_chunk.prev_foot = psize;     /* set_foot(p, psize) */
              goto postaction;
              end; /* else */
            end; /* ok_address */
          else do;
	    display( 'Heap storage error 4' );                  /*0.9.2*/
            goto erroraction;
	    end;
          end; /* ^pinuse */

        /********************************************************/
        /* See if this chunk can be consolidated with the       */
	/* following one                                        */
        /********************************************************/
        /* if (RTCHECK(ok_next(p, next) && ok_pinuse(next))) {  */
        /* #define ok_next(p, n)    ((char*)(p) < (char*)(n))   */
        /* #define ok_pinuse(p)     pinuse(p)                   */
        if p<next & IAND(next->malloc_chunk.head,PINUSE_BIT)^=0
	then do;
	  /* Is the following chunk in use?                     */
          if IAND(next->malloc_chunk.head,CINUSE_BIT)=0
          then do;
            /* Next chunk is top chunk */
            if next = malloc_state.top then do;
	      tsize,malloc_state.topsize = malloc_state.topsize + psize;
              malloc_state.top = p;
              /* p->head = tsize | PINUSE_BIT; */
              p->malloc_chunk.head = IOR(tsize,PINUSE_BIT);
	      /* If this chunk is also the designated victim, */
	      /* clear the dv info in malloc_state            */
              if p = malloc_state.dv then do;
                malloc_state.dv = sysnull();
                malloc_state.dvsize = 0;
                end; /* p=gm->dv */
              /* #define should_trim(M,s)  ((s) > (M)->trim_check) */
	      /* Check to see if we need to trim memory */
              if tsize > malloc_state.trim_check    /* should_trim(gm, tsize) */
              then call sys_trim(0);
              goto postaction;
              end; /* next=gm->top */
	    /* If next is not the top chunk, check  */
	    /* to see if it's the designated victim */
            else if next = malloc_state.dv then do;
              /* Next chunk is designated victim */
              dsize,malloc_state.dvsize = malloc_state.dvsize + psize;
              malloc_state.dv = p;
              /* set_size_and_pinuse_of_free_chunk(p, dsize) */
              p->malloc_chunk.head = IOR(dsize,PINUSE_BIT);
              ptradd(p,dsize)->malloc_chunk.prev_foot = dsize;
              goto postaction;
              end; /* next=gm->dv */
	    /* Otherwise the next chunk is free, but not 'special' */
	    /* so just consolidate                                 */
            else do;
              nsize = chunksize(next);
              psize = psize + nsize;
              /* unlink_chunk(fm, next, nsize); */
              /* #define unlink_chunk(M, P, S)
              /*   if (is_small(S)) unlink_small_chunk(M, P, S) */
              /*   else { tchunkptr TP = (tchunkptr)(P); unlink_large_chunk(M, TP) */
	      if ISRL(nsize,SMALLBIN_SHIFT) < NSMALLBINS /* is_small(nsize) */
              then call unlink_small_chunk(next,nsize);
              else call unlink_large_chunk(next);
              /* set_size_and_pinuse_of_free_chunk(p, psize); */
              p->malloc_chunk.head = IOR(psize,PINUSE_BIT);
              ptradd(p,psize)->malloc_chunk.prev_foot = psize;
              if p = malloc_state.dv then do;
                malloc_state.dvsize = psize;
                goto postaction;
                end; /* p=gm->dv */
              end; /* else */
           end; /* ^cinuse */
	 /* Otherwise the next chunk is in use     */
        else do; /* cinuse */
          /* set_free_with_pinuse(p, psize, next); */
          /* #define set_free_with_pinuse(p, s, n) */
          /* (clear_pinuse(n), set_size_and_pinuse_of_free_chunk(p, s)) */
          /* #define clear_pinuse(p)     ((p)->head &= ~PINUSE_BIT) */
          /* #define set_size_and_pinuse_of_free_chunk(p, s) */
          /*   ((p)->head = (s|PINUSE_BIT), set_foot(p, s))  */
          /* #define set_foot(p, s)  (((mchunkptr)((char*)(p) + (s)))->prev_foot = (s)) */
          next->malloc_chunk.head = IAND(next->malloc_chunk.head,INOT(PINUSE_BIT));  /* clear_pinuse(next) */
          p->malloc_chunk.head = IOR(psize,PINUSE_BIT);            /* set_size_and_pinuse_of_free_chunk(p, s) */
          ptradd(p,psize)->malloc_chunk.prev_foot = psize;         /* set_foot(p, psize) */
	  end; /* cinuse */
	   
	/* Insert freed chunk in free list */  
        if ISRL(psize,SMALLBIN_SHIFT) < NSMALLBINS /* is_small(psize) */
        then call insert_small_chunk(p, psize);
        else do;
          call insert_large_chunk(p, psize);
          malloc_state.release_checks = malloc_state.release_checks-1;
          if malloc_state.release_checks = 0
          then call release_unused_segments();
          end; /* else */
        goto postaction;
        end; /* ^ok_pinuse */
      end; /* ok_address */
    display( 'Heap storage error 5' );
    display( 'Freeing '||heximage(addr(mem),4) );
erroraction:       
    call release_lock( addr(malloc_state.mutex) );
/* NOTE: SIGNAL here causes recursive error                     1.2.0*/
/*  signal ERROR;                                           /*0.9.10e*/
    call OSExit;                        /* NOTE: recursive error     */
  postaction:
      call release_lock( addr(malloc_state.mutex) );
      
      end; /* ^preaction */
      
    end; /* mem^=sysnull */

/* Unmap and unlink any mmapped segments that don't contain used chunks */
release_unused_segments: proc returns( fixed bin(31) );
  dcl    released            fixed bin(31)  init(0);   /* size_t */
  dcl    nsegs               fixed bin(31)  init(0);   /* int    */
  dcl    pred                ptr;                      /* msegmentptr */
  dcl    sp                  ptr;                      /* msegmentptr */
  dcl    base                ptr;                      /* char*  */
  dcl    size                fixed bin(31);            /* size_t */
  dcl    next                ptr;                      /* msegmentptr */

  pred = addr(malloc_state.seg);
  sp = pred->malloc_segment.next;
  do while (sp ^= sysnull());
    base = sp->malloc_segment.base;
    size = sp->malloc_segment.size;
    next = sp->malloc_segment.next;
    nsegs=nsegs+1;
    pred = sp;
    sp = next;
    end; /* do while */
  /* Reset check counter */
  /* m->release_checks = ((nsegs > MAX_RELEASE_CHECK_RATE)? */
  /*                      nsegs : MAX_RELEASE_CHECK_RATE);  */
  if nsegs>MAX_RELEASE_CHECK_RATE
  then malloc_state.release_checks = MAX_RELEASE_CHECK_RATE;
  else malloc_state.release_checks = nsegs; 
  return( released );
  end release_unused_segments;

  end internal_dlfree;

/*-----------------------------------------------*/
/* init_mparams                                  */
/*-----------------------------------------------*/
init_mparams: proc returns( fixed bin(31) );
  dcl     s                    fixed bin(31);
  if mparams.page_size=0 then do;
    mparams.mmap_threshold = DEFAULT_MMAP_THRESHOLD;
    mparams.trim_threshold = DEFAULT_TRIM_THRESHOLD;
    mparams.default_mflags = 2;        /* USE_LOCK_BIT|USE_MMAP_BIT; */
    s = IEOR( c_time(0), 1431655765 ); /* s = (time(0) ^ 0x55555555U) */
    s = IOR(s,8);                      /* s |= 8       ensure nonzero */
    s = IAND(s,INOT(7));               /* s &= ~7  improve chances of fault for bad values */
    call acquire_lock( addr(magic_init_mutex) ); /* ACQUIRE_MAGIC_INIT_LOCK() */
    if mparams.magic = 0 then do;
      mparams.magic = s;
      /* Set up lock for main malloc area */
      /* INITIAL_LOCK(&gm->mutex) */
      call mutex_init( addr(malloc_state.mutex), 0 );         /*0.9.2*/		    
      malloc_state.mflags = mparams.default_mflags;
      end; /* magic=0 */
    call release_lock( addr(magic_init_mutex) ); /* RELEASE_MAGIC_INIT_LOCK() */

    mparams.page_size = 4096;        /* malloc_getpagesize */
    /* mparams.granularity = ((DEFAULT_GRANULARITY != 0)?               */
    /*                        DEFAULT_GRANULARITY : mparams.page_size); */
    mparams.granularity = DEFAULT_GRANULARITY;
    /*
       top_foot_size is padding at the end of a segment, including space
       that may be needed to place segment records and fenceposts when new
       noncontiguous segments are added.
     */
     /* #define top_foot_size
      *   (align_offset(chunk2mem(0))+pad_request(sizeof(struct malloc_segment))+MIN_CHUNK_SIZE)
      */
    top_foot_size = align_offset(ptrvalue(8))
      + IAND(stg(null()->malloc_segment)+CHUNK_OVERHEAD+CHUNK_ALIGN_MASK,INOT(CHUNK_ALIGN_MASK))
      + MIN_CHUNK_SIZE;
    /* Sanity-check configuration:
       size_t must be unsigned and as wide as pointer type.
       ints must be at least 4 bytes.
       alignment must be at least 8.
       Alignment, min chunk size, and page size must all be powers of 2.
    */
    if  (MAX_SIZE_T < MIN_CHUNK_SIZE)  | 
        (MALLOC_ALIGNMENT < 8)         |
        /* (MALLOC_ALIGNMENT    & (MALLOC_ALIGNMENT-1))   ^= 0  | */
        /* (MCHUNK_SIZE         & (MCHUNK_SIZE-1)         ^= 0) | */
        /* (mparams.granularity & (mparams.granularity-1) ^= 0) | */
        /* (mparams.page_size   & (mparams.page_size-1)   ^= 0)   */
        IAND( IOR(MALLOC_ALIGNMENT,0), IOR(MALLOC_ALIGNMENT-1,0) ) ^= 0 | 
        IAND( IOR(stg(null()->malloc_chunk),0), IOR(stg(null()->malloc_chunk)-1,0) )  ^= 0 |
        IAND( IOR(mparams.granularity,0), IOR(mparams.granularity-1,0) )  ^= 0 |
        IAND( IOR(mparams.page_size,0), IOR(mparams.page_size-1,0) )  ^= 0
    then signal ERROR;
    end; /* page_size=0 */
  return(0);
  
  end init_mparams;

/*-----------------------------------------------*/
/* sys_alloc: allocate memory                    */
/*-----------------------------------------------*/
sys_alloc: proc(nb) returns( ptr );
  dcl     nb                  fixed bin(31); /* size_t */
  dcl     tbase               ptr;           /* char*  */
  dcl     tsize               fixed bin(31); /* size_t */
  dcl     br                  ptr;           /* char*  */
  dcl     asize               fixed bin(31); /* size_t */
  dcl     ss                  ptr;           /* msegmentptr */
  dcl     base                ptr;           /* char*  */
  dcl     esize               fixed bin(31); /* size_t */
  dcl     end                 ptr;           /* char*  */
  dcl     ssize               fixed bin(31); /* size_t */
  dcl     mn                  ptr;           /* mchunkptr */
  dcl     sp                  ptr;           /* msegmentptr */
  dcl     oldbase             ptr;           /* char*  */
  dcl     rsize               fixed bin(31); /* size_t */
  dcl     p                   ptr;           /* mchunkptr */
  dcl     r                   ptr;           /* mchunkptr */
  dcl     i                   fixed bin(31); /* bindex_t */
  dcl     bin                 ptr;           /* sbinptr  */
  
  tbase = CMFAIL;
  tsize = 0;

  call init_mparams();
  
  /*
    Try getting memory in any of two ways (in most-preferred to
    least-preferred order):
    1. A call to MORECORE that can normally contiguously extend memory.
       (disabled if not MORECORE_CONTIGUOUS or not HAVE_MORECORE or
       or main space is mmapped or a previous contiguous call failed)
    2. A call to MORECORE that cannot usually contiguously extend memory.
       (disabled if not HAVE_MORECORE)
  */

  /************************************/
  /* Try to contiguously extend memory*/
  /************************************/
  /* if ^use_noncontiguous(m) then do; */
  if IAND(malloc_state.mflags,USE_NONCONTIGUOUS_BIT)=0 then do;
    br = CMFAIL;
    /* ss = (m->top == 0)? 0 : segment_holding(m, (char*)m->top); */
    if malloc_state.top=sysnull()
    then ss = segment_holding(malloc_state.top);
    else ss = sysnull();
    asize = 0;
    call acquire_lock( addr(morecore_mutex) ); /* ACQUIRE_MORECORE_LOCK(); */
    if ss = sysnull() then do; /* First time through or recovery */
      base = morecore(0);
      if base ^= CMFAIL then do;
        /* #define granularity_align(S)    */
        /*  (((S) + (mparams.granularity - SIZE_T_ONE)) */
        /*    & ~(mparams.granularity - SIZE_T_ONE))    */
        /* asize = granularity_align(nb + top_foot_size + 1) */
        asize = nb+top_foot_size+1;
        asize = (asize + mparams.granularity-1)/mparams.granularity;
        asize = asize * mparams.granularity;
        /*asize = asize + IAND( mparams.granularity-1, INOT(mparams.granularity-1) );*/
        /* Adjust to end on a page boundary */
        /* #define is_page_aligned(S) */
        /*   (((size_t)(S) & (mparams.page_size - SIZE_T_ONE)) == 0) */
        /* if ^is_page_aligned(base) */
        /* #define page_align(S)\ */
        /*   (((S) + (mparams.page_size - SIZE_T_ONE)) & ~(mparams.page_size - SIZE_T_ONE)) */
        /* if (!is_page_aligned(base)) */
        /*   asize += (page_align((size_t)base) - (size_t)base); */
	if IAND(binvalue(base),mparams.page_size-1)^=0
        then asize = asize +
	       IAND( binvalue(base) + mparams.page_size-1,
		     INOT(mparams.page_size-1)) - binvalue(base);
        /* Can't call MORECORE if size is negative when treated as signed */
        if asize < HALF_MAX_SIZE_T then do;                         /*20100701*/
          br = morecore(asize);                                     /*20100701*/
          if br = base then do;                                     /*20100701*/
            tbase = base;
            tsize = asize;
            end;                                                    /*20100701*/
	  end; /* asize */                                          /*20100701*/
        end; /* base */
      end; /* ss */
    else do;
      /* Subtract out existing available top space from MORECORE request. */
      /* asize = granularity_align(nb - m->topsize + TOP_FOOT_SIZE + SIZE_T_ONE) */
      asize = nb-malloc_state.topsize+top_foot_size;
      asize = (asize + mparams.granularity-1)/mparams.granularity;
      asize = asize * mparams.granularity;
      /* asize = asize + IAND( mparams.granularity-1, INOT(mparams.granularity-1) ); */
      /* Use mem here only if it did continuously extend old space */
      if asize < HALF_MAX_SIZE_T then do;
        br = morecore(asize);
        if br  = ss->malloc_segment.base+ss->malloc_segment.size then do;
          tbase = br;
          tsize = asize;
          end; /* asize */
	end;
      end; /* else */

    if tbase = CMFAIL then do;    /* Cope with partial failure */
      if br ^= CMFAIL then do;    /* Try to use/extend the space we did get */
        if asize < HALF_MAX_SIZE_T &
           asize < nb + top_foot_size + 1 then do;
          /* esize = granularity_align(nb + top_foot_size + 1 - asize) */
          esize = nb+top_foot_size+1;
          esize = (esize + mparams.granularity-1)/mparams.granularity;
          esize = esize * mparams.granularity;
          /* size = esize + IAND( mparams.granularity-1, INOT(mparams.granularity-1) ); */
          if esize < HALF_MAX_SIZE_T then do;
            end = morecore(esize);
            if end ^= CMFAIL
            then asize = asize + esize;
            else do;            /* Can't use; try to release */
              br = morecore(-asize);
              br = CMFAIL;
              end;
            end;
         end;
       end;
      if br ^= CMFAIL then do;    /* Use the space we did get */
        tbase = br;
        tsize = asize;
        end;
      else
        /* #define disable_contiguous(M) ((M)->mflags |=  USE_NONCONTIGUOUS_BIT) */
        /*disable_contiguous(m)  Don't try contiguous path in the future */
        malloc_state.mflags = IOR(malloc_state.mflags,USE_NONCONTIGUOUS_BIT);
      end; 
    call release_lock( addr(morecore_mutex) );   /* RELEASE_MORECORE_LOC0.9.2*/

    end; /* ^USE_NONCONTIGUOUS */

  /************************************/
  /* Try noncontiguous MORECORE       */
  /************************************/
  if tbase = CMFAIL then do; /* Try noncontiguous MORECORE */
    /* asize = granularity_align(nb + top_foot_size + 1) */
    asize = IAND( asize + mparams.granularity-1, INOT(mparams.granularity-1) );   
    if asize < HALF_MAX_SIZE_T then do; /* HALF_MAX_SIZE_T */
      br = CMFAIL;
      end = CMFAIL;
      call acquire_lock( addr(morecore_mutex) ); /* ACQUIRE_MORECORE_LOCK(); */
      br  = morecore(asize);
      end = morecore(0);
      call release_lock( addr(morecore_mutex) ); /* RELEASE_MORECORE_LOCK(); */
      if br ^= CMFAIL & end ^= CMFAIL & br < end then do;
        ssize = end - br;
        if ssize > nb + top_foot_size then do;
          tbase = br;
          tsize = ssize;
          end; /* ssize */
        end; /* br */
      end; /* asize */
    end; /* tbase */

  /************************************/
  /* Have memory now                  */
  /************************************/
  if tbase ^= CMFAIL then do;
    malloc_state.footprint = malloc_state.footprint + tsize;
    if malloc_state.footprint > malloc_state.max_footprint
    then malloc_state.max_footprint = malloc_state.footprint;
    if malloc_state.top=sysnull() then do; /* ^is_initialized(m) - first-time initialization */
      seg.base,malloc_state.least_addr = tbase;
      seg.size = tsize;
      seg.sflags = 0;
      malloc_state.magic = mparams.magic;
      malloc_state.release_checks = MAX_RELEASE_CHECK_RATE;
      /* init_bins(m); */
      /* Establish circular links for smallbins */
      do i=0 to NSMALLBINS-1;    /* for (i = 0; i < NSMALLBINS; ++i) */
        /* #define smallbin_at(M, i)   ((sbinptr)((char*)&((M)->smallbins[(i)<<1]))) */
        bin = addr(malloc_state.smallbins(ISLL(i,1)));   /* smallbin_at(m, i) */
        bin->malloc_chunk.fd,bin->malloc_chunk.bk = bin;
        end;
      call init_top(tbase, tsize - top_foot_size);
      end; /* ^is_initialized */
    else do;
      /* Try to merge with an existing segment */
      sp = addr(malloc_state.seg);
      /* Only consider most recent segment if traversal suppressed */
      do while( sp ^= sysnull()  & tbase ^= sp->malloc_segment.base + sp->malloc_segment.size );
	sp = sp->malloc_segment.next;
	end; /* do while */
      /* if (sp != 0 &&                 */
      /*     !is_extern_segment(sp) &&  */
      /*     (sp->sflags & IS_MMAPPED_BIT) == mmap_flag && */
      /*     segment_holds(sp, m->top)) { /* append */
      /*#define segment_holds(S, A)\ */
      /*  ((char*)(A) >= S->base && (char*)(A) < S->base + S->size) */
      if malloc_state.top>=sp->malloc_segment.base &
         malloc_state.top<sp->malloc_segment.base+sp->malloc_segment.size
      then do;
        sp->malloc_segment.size = sp->malloc_segment.size + tsize;
        call init_top(malloc_state.top, malloc_state.topsize+tsize);
        end;
      else do;
        if tbase < malloc_state.least_addr
        then malloc_state.least_addr = tbase;
        sp = addr(malloc_state.seg);
        do while( sp ^= sysnull() & 
                sp->malloc_segment.base ^= tbase + tsize);
          sp = sp->malloc_segment.next;
          end;
        if sp^=sysnull then do;
          oldbase = sp->malloc_segment.base;
          sp->malloc_segment.base = tbase;
          sp->malloc_segment.size = sp->malloc_segment.size + tsize;
          return( prepend_alloc(tbase, oldbase, nb) );
          end;
        else call add_segment(tbase, tsize);
        end; /* else */
      end; /* merge */

    if nb < malloc_state.topsize then do; /* Allocate from new or extended top space */
      rsize,malloc_state.topsize = malloc_state.topsize - nb;
      p = malloc_state.top;
      r,malloc_state.top = p+nb;         /* chunk_plus_offset(p, nb) */
      /* r->head = rsize | PINUSE_BIT; */
      r->malloc_chunk.head = IOR(rsize,PINUSE_BIT);
      /* set_size_and_pinuse_of_inuse_chunk(m, p, nb); */
      p->malloc_chunk.head = IOR(nb,INUSE_BITS);
      /* check_top_chunk(m, m->top);                      DEBUG only */
      /* check_malloced_chunk(m, p+8 (*chunk2mem(p)*), nb) DEBUG only*/
      return( p+8 /*chunk2mem(p)*/ ); 
      end; /* nb<m->topsize */

    end; /* tbase^=CMFAIL */

  call release_lock( addr(malloc_state.mutex) );              /*0.9.3*/
  signal STORAGE;                      /* No more memory available   */
  
  end sys_alloc;

/*-----------------------------------------------*/
/* sys_trim: Trim unused memory                  */
/*-----------------------------------------------*/
sys_trim: proc(pad) returns( fixed bin(31) );
  dcl    pad                 fixed bin(31);            /* size_t */
  dcl    released            fixed bin(31)  init(0);   /* size_t */
  dcl    unit                fixed bin(31);            /* size_t */
  dcl    extra               fixed bin(31);            /* size_t */
  dcl    sp                  ptr;                      /* msegmentptr */
  dcl   (old_br,rel_br,new_br)      
                             ptr;                      /* char* */
			     
  if pad < MAX_REQUEST & malloc_state.top^=sysnull() then do; /* is_initialized(m) */
    pad = pad + top_foot_size; /* ensure enough room for segment overhead */
    if malloc_state.topsize > pad then do;
      /* Shrink top space in granularity-size units, keeping at least one */
      unit = mparams.granularity;
      /* size_t extra = ((m->topsize - pad + (unit - SIZE_T_ONE)) / unit - */
      /*                 SIZE_T_ONE) * unit; 
(nofovl):                                    */    
      extra = ((malloc_state.topsize - pad + (unit-1)) / unit  - 1 ) * unit;
      sp = segment_holding(malloc_state.top); 
      if extra >= HALF_MAX_SIZE_T /* Avoid wrapping negative */
      then extra = (HALF_MAX_SIZE_T) + 1 - unit;
      call acquire_lock( addr(morecore_mutex) ); /* ACQUIRE_MORECORE_LOCK(); */
      /* Make sure end of memory is where we last set it. */
      old_br = morecore(0);
      if old_br = sp->malloc_segment.base + sp->malloc_segment.size then do;
        rel_br = morecore(-extra);
        new_br = morecore(0);
        if rel_br ^= CMFAIL & new_br < old_br
        then released = old_br - new_br;
        end; /* old_br */
      call release_lock( addr(morecore_mutex) );   /* RELEASE_MORECORE_LOCK() */
      end;

      if released^= 0 then do;
        sp->malloc_segment.size = sp->malloc_segment.size - released;
        malloc_state.footprint = malloc_state.footprint - released;
        call init_top(malloc_state.top, malloc_state.topsize - released);
        /* check_top_chunk(m, m->malloc_state.top) */
        /* do_check_top_chunk(malloc_state.top) */
        end; /* released^=0 */

    /* On failure, disable autotrim to avoid repeated failed future calls */
    if released = 0 & malloc_state.topsize > malloc_state.trim_check
    then malloc_state.trim_check = MAX_SIZE_T;
    end;

  if released^=0   /* return (released != 0)? 1 : 0 */
  then return(1);
  else return(0);
  end sys_trim;

/* Allocate chunk and prepend remainder with chunk in successor base. */
prepend_alloc: proc(newbase, oldbase, nb) returns( ptr );
  dcl   (newbase,oldbase)    ptr;                /* char* */
  dcl    nb                  fixed bin(31);      /* size_t */
  dcl   (p,q,oldfirst)       ptr;                /* mchunkptr */
  dcl   (psize,qsize)        fixed bin(31);      /* size_t */
  dcl   (tsize,dsize,nsize)  fixed bin(31);      /* size_t */
               
  /* p = align_as_chunk(newbase); */
  p = newbase + align_offset(newbase+8);
  /* oldfirst = align_as_chunk(oldbase); */
  oldfirst = oldbase + align_offset(oldbase+8);
  psize = oldfirst - p;
  q = p+nb; /* chunk_plus_offset(p, nb) */
  qsize = psize - nb;
  /* set_size_and_pinuse_of_inuse_chunk(m, p, nb) */
  p->malloc_chunk.head = IOR(nb,INUSE_BITS);

  /* consolidate remainder with first chunk of old base */
  if oldfirst = malloc_state.top then do;
    tsize, malloc_state.topsize = malloc_state.topsize + qsize;
    malloc_state.top = q;
    q->malloc_chunk.head = IOR(tsize,PINUSE_BIT);
    /* check_top_chunk(m, q) */
    end;
  else if oldfirst = malloc_state.dv then do;
    dsize, malloc_state.dvsize = malloc_state.dvsize + qsize;
    malloc_state.dv = q;
    /* set_size_and_pinuse_of_free_chunk(q, dsize) */
    q->malloc_chunk.head = IOR(dsize,PINUSE_BIT);
    ptradd(q,dsize)->malloc_chunk.prev_foot = dsize;
    end;
  else do;
    if IAND(binvalue(oldfirst),CINUSE_BIT)=0 then do; /* !cinuse(oldfirst) */
      nsize = chunksize(oldfirst);
      /* unlink_chunk(oldfirst, nsize) */
      if ISRL(nsize,SMALLBIN_SHIFT) < NSMALLBINS
      then call unlink_small_chunk(oldfirst,nsize);
      else call unlink_large_chunk(oldfirst);
      oldfirst = oldfirst+nsize; /* chunk_plus_offset(oldfirst, nsize) */
      qsize = qsize + nsize;
      end;
    /* set_free_with_pinuse(q, qsize, oldfirst); */
    oldfirst->malloc_chunk.head = IAND(oldfirst->malloc_chunk.head,INOT(PINUSE_BIT));
    q->malloc_chunk.head = IOR(qsize,PINUSE_BIT);
    ptradd(q,qsize)->malloc_chunk.prev_foot = qsize;
    /* insert_chunk(m, q, qsize); */
    if ISRL(qsize,SMALLBIN_SHIFT) < NSMALLBINS
    then call insert_small_chunk(q,qsize);
    else call insert_large_chunk(q,qsize);
    /* check_free_chunk(m, q) */
    end; /*else */

  /* check_malloced_chunk(m, chunk2mem(p), nb) */
  return(p+8);    /* chunk2mem(p) */
  end prepend_alloc;

/* Add a segment to hold a new noncontiguous region */
add_segment: proc(tbase,tsize);
  dcl    tbase               ptr;                /* char*  */
  dcl    tsize               fixed bin(31);      /* size_t */
  dcl   (old_top,old_end)    ptr;                /* char*  */
  dcl   (rawsp,asp,csp)      ptr;                /* char*  */
  dcl    oldsp               ptr;                /* msegmentptr */
  dcl    ssize               fixed bin(31);      /* size_t */
  dcl    offset              fixed bin(31);      /* size_t */
  dcl    sp                  ptr;                /* mchunkptr */
  dcl    ss                  ptr;                /* msegmentptr */
  dcl   (p,tnext)            ptr;                /* mchunkptr */
  dcl    nfences             fixed bin(31);      /* int */
  dcl    nextp               ptr;                /* mchunkptr */
  dcl    q                   ptr;                /* mchunkptr */
  dcl    psize               fixed bin(31);      /* size_t */
  dcl    tn                  ptr;                /* mchunkptr */

  /* Determine locations and sizes of segment, fenceposts, old top */
  old_top = malloc_state.top;
  oldsp = segment_holding(old_top);
  old_end = oldsp->malloc_segment.base + oldsp->malloc_segment.size;
  /* #define pad_request(req)  */
  /*    (((req) + CHUNK_OVERHEAD + CHUNK_ALIGN_MASK) & ~CHUNK_ALIGN_MASK) */
  /* ssize = pad_request(sizeof(struct malloc_segment)) */
  ssize = IAND(stg(null()->malloc_segment)+CHUNK_OVERHEAD+CHUNK_ALIGN_MASK,INOT(CHUNK_ALIGN_MASK)); /* pad_request(bytes); */
  rawsp = old_end - (ssize + 16 + CHUNK_ALIGN_MASK);
  offset = align_offset(rawsp+8);
  asp = rawsp + offset;
  /* csp = (asp < (old_top + MIN_CHUNK_SIZE))? old_top : asp */
  if asp<old_top+MIN_CHUNK_SIZE
  then csp = asp;
  else csp = old_top;
  sp = csp;
  ss = sp+8;
  tnext = sp+ssize; /* chunk_plus_offset(sp, ssize) */
  p = tnext;
  nfences = 0;

  /* reset top to new space */
  call init_top(tbase, tsize - top_foot_size);

  /* Set up segment record */
  /* set_size_and_pinuse_of_inuse_chunk(sp, ssize) */
  sp->malloc_chunk.head = IOR(ssize,INUSE_BITS);
  ss->malloc_segment = malloc_state.seg; /* Push current record */
  malloc_state.seg.base = tbase;
  malloc_state.seg.size = tsize;
  malloc_state.seg.sflags = 0;   /* memmapped */
  malloc_state.seg.next = ss;

  /* Insert trailing fenceposts */
  do while( '1'b );   /* for (;;) */
    nextp = p+4; /* chunk_plus_offset(p,4) */
    p->malloc_chunk.head = FENCEPOST_HEAD;
    nfences = nfences + 1;
    if addr(nextp->malloc_chunk.head) < old_end
    then p = nextp;
    else leave; /* break */
    end;

  /* Insert the rest of old top into a bin as an ordinary free chunk */
  if csp ^= old_top then do;
    q = old_top;
    psize = csp - old_top;
    tn = q+psize; /* chunk_plus_offset(q, psize) */
    /* set_free_with_pinuse(q, psize, tn) */
    tn->malloc_chunk.head = IAND(tn->malloc_chunk.head,INOT(PINUSE_BIT));  /* clear_pinuse(tn) */
    p->malloc_chunk.head = IOR(psize,PINUSE_BIT);        /* set_size_and_pinuse_of_free_chunk(q, s) */
    ptradd(p,psize)->malloc_chunk.prev_foot = psize;     /* set_foot(q, psize) */
    /* insert_chunk(m, q, psize); */    
    /* #define insert_chunk(M, P, S)\ */
    /*   if (is_small(S)) insert_small_chunk(M, P, S)\ */
    /*   else { tchunkptr TP = (tchunkptr)(P); insert_large_chunk(M, TP, S); } */
    /* #define is_small(s)(((s) >> SMALLBIN_SHIFT) < NSMALLBINS) */
    if ISRL(psize,SMALLBIN_SHIFT) < NSMALLBINS  /* if is_small(psize) */
    then call insert_small_chunk(q,psize);
    else call insert_large_chunk(q,psize);
    end;
  /* check_top_chunk(m, m->top) */
  /* do_check_top_chunk(m, m->malloc_state.top) */
  end add_segment;
    
/* Return segment holding given address */
segment_holding: proc(xaddr) returns(ptr);        /* msegmentptr */
  dcl     xaddr               ptr;                /* char*  */
  dcl     sp                  ptr;                /* msegmentptr */
  sp = addr(malloc_state.seg);
  do while ('1'b );        /* for (;;)  */
    if xaddr >= sp->malloc_segment.base & 
       xaddr < sp->malloc_segment.base + sp->malloc_segment.size
    then return( sp );
    sp = sp->malloc_segment.next;
    if sp = sysnull()
    then return( sysnull() );
    end; /* do while */
  end segment_holding;
  
 /* Extract the chunk size from the header.  Given the fact that    */
 /* we're keeping the same format as the C version, this is         */
 /* markedly less efficient in PL/I/                                */
 chunksize: proc(p) returns( fixed bin(31) );
   dcl   p                   ptr;                /* tchunkptr */
   dcl   temp                fixed bin(31);
   temp = IAND(p->malloc_chunk.head,INOT(FLAG_BITS));
   return( temp );
   end chunksize;

/* Replace dv node, binning the old one */
/* Used only when dvsize known to be small */
/* #define replace_dv(M, P, S) {\
 *   size_t DVS = M->dvsize;\
 *   if (DVS != 0) {\
 *     mchunkptr DV = M->dv;\
 *     assert(is_small(DVS));\
 *     insert_small_chunk(M, DV, DVS);\
 *   }\
 *   M->dvsize = S;\
 *   M->dv = P;\
 * }
 */
 replace_dv: proc(pV,size);
   dcl   pV                  ptr;           /* addr(designated victim) */
   dcl   size                fixed bin(31); /* size_t */
   dcl   dv                  ptr;           /* mchunkptr */
   dcl   dvs                 fixed bin(31); /* size_t */
   dvs = malloc_state.dvsize;
   if dvs^=0 then do;
     dv = malloc_state.dv;
     call insert_small_chunk(dv,dvs);
     end;
   malloc_state.dvsize = size;
   malloc_state.dv     = pV;
   end replace_dv;

/* assign tree index for size S to variable I */
/* #define compute_tree_index(S, I)\
 * {\
 *   size_t X = S >> TREEBIN_SHIFT;\
 *   if (X == 0)\
 *     I = 0;\
 *  else if (X > 0xFFFF)\
 *     I = NTREEBINS-1;\
 * 
 *   else {\
 *     unsigned int K;\
 *     _BitScanReverse((DWORD *) &K, X);\
 *     I =  (bindex_t)((K << 1) + ((S >> (K + (TREEBIN_SHIFT-1)) & 1)));\
 *   }\
 * }
 */
compute_tree_index: proc(s,i) returns( fixed bin(31) );
  dcl    s                   fixed bin(31);
  dcl    i                   fixed bin(31); /* size_t */
  dcl    x                   fixed bin(31); /* size_t */
  dcl    k                   fixed bin(31); /* unsigned int */
  x = ISRL(s,TREEBIN_SHIFT);
  if x=0 then i=0;
  else if x>65535 then i = NTREEBINS-1; /* 65535=0xFFFF */
  else do;
    call BitScanReverse(k,x);
    i = ISLL(k,1) + IAND(ISRL(s,k+TREEBIN_SHIFT-1),1);
    end;
  return(i);
  end compute_tree_index;

/* Initialize top chunk and its size */
init_top: proc(px,psizex);
  dcl    px                  ptr;                /* mchunkptr */
  dcl    psizex              fixed bin(31);      /* size_t */
  dcl    offset              fixed bin(31);      /* size_t */
  dcl    p                   ptr;                /* mchunkptr */
  dcl    psize               fixed bin(31);      /* size_t */
  /* Ensure alignment */
  p = px;                               /* Save argument data  */
  psize = psizex;
  offset = align_offset(p+8 /*chunk2mem(p)*/ );
  p = p + offset;
  psize = psize - offset;
  malloc_state.top = p;
  malloc_state.topsize = psize;
  p->malloc_chunk.head = IOR(psize,PINUSE_BIT);
  /* set size of fake trailing chunk holding overhead space only once */
  ptradd(p,psize)->malloc_chunk.head = top_foot_size;
  malloc_state.trim_check = mparams.trim_threshold; /* reset on each update */
  end init_top;
  
/* Insert chunk into tree */
/* #define insert_large_chunk(M, X, S) {\
 *   tbinptr* H;\
 *   bindex_t I;\
 *   compute_tree_index(S, I);\
 *   H = treebin_at(M, I);\
 *   X->index = I;\
 *   X->child[0] = X->child[1] = 0;\
 *   if (!treemap_is_marked(M, I)) {\
 *     mark_treemap(M, I);\
 *     *H = X;\
 *     X->parent = (tchunkptr)H;\
 *     X->fd = X->bk = X;\
 *   }\
 *   else {\
 *     tchunkptr T = *H;\
 *     size_t K = S << leftshift_for_tree_index(I);\
 *     for (;;) {\
 *       if (chunksize(T) != S) {\
 *         tchunkptr* C = &(T->child[(K >> (SIZE_T_BITSIZE-SIZE_T_ONE)) & 1]);\
 *         K <<= 1;\
 *         if (*C != 0)\
 *           T = *C;\
 *         else if (RTCHECK(ok_address(M, C))) {\
 *           *C = X;\
 *           X->parent = T;\
 *           X->fd = X->bk = X;\
 *           break;\
 *         }\
 *         else {\
 *           CORRUPTION_ERROR_ACTION(M);\
 *           break;\
 *         }\
 *       }\
 *       else {\
 *         tchunkptr F = T->fd;\
 *         if (RTCHECK(ok_address(M, T) && ok_address(M, F))) {\
 *           T->fd = F->bk = X;\
 *           X->fd = F;\
 *           X->bk = T;\
 *           X->parent = 0;\
 *           break;\
 *         }\
 *         else {\
 *           CORRUPTION_ERROR_ACTION(M);\
 *           break;\
 *         }\
 *       }\
 *     }\
 *   }\
 * 
 */
insert_large_chunk: proc(x, s);
  dcl     x                   ptr;                /* tchunkptr */
  dcl     s                   fixed bin(31);      /* size_t? */
  dcl     h                   ptr;                /* tbinptr* */
  dcl     i                   fixed bin(31);      /* bindex_t */
  dcl     t                   ptr;                /* tchunkptr */
  dcl     k                   fixed bin(31);      /* size_t */
  dcl     c                   ptr;                /* tchunkptr* */
  dcl     f                   ptr;                /* tchunkptr */
  dcl     shft                fixed bin(31);      /* PL/I */
  dcl     dummy               fixed bin(31);
  
  dummy = compute_tree_index(S, i);
  /* #define treebin_at(M,i)     (&((M)->treebins[i])) */
  h = addr(malloc_state.treebins(i));   /* treebin_at(M, I) */
  x->malloc_tree_chunk.index = I;
  x->malloc_tree_chunk.child(0),X->malloc_tree_chunk.child(1) = sysnull();
  x->malloc_tree_chunk.parent = sysnull(); /* PRF */
  /* if ^treemap_is_marked(M, I) then do; */
  /* #define treemap_is_marked(M,i)  ((M)->treemap  &   idx2bit(i)) */
  if ^IAND(malloc_state.treemap,ISLL(1,i)) then do;
     /* First entry for this size         */
    /* #define mark_treemap(M,i) ((M)->treemap  |=  idx2bit(i)) */
    /* mark_treemap(M, I); */
    malloc_state.treemap = IOR(malloc_state.treemap,ISLL(1,I));
    h->aPtr = x;         /* *H = X; */
    x->malloc_tree_chunk.parent = H;
    x->malloc_tree_chunk.fd, x->malloc_tree_chunk.bk = X;
    end;
  else do;
    t = h->aPtr;           /* *H (root of tree) */
    /* size_t K = S << leftshift_for_tree_index(I); */
    /* #define leftshift_for_tree_index(i) \ */
    /*    ((i == NTREEBINS-1)? 0 : \         */
    /* ((SIZE_T_BITSIZE-SIZE_T_ONE) - (((i) >> 1) + TREEBIN_SHIFT - 2))) */
    /* Shift the MSB of the current chunk size into */
    /* the MSB of a word                            */
    if I=NTREEBINS-1 then shft = 0;
    else shft = 31-(ISRL(I,1)+6);
    K = ISLL(S,shft);
 for:   do while( '1'b );         /* for (;;) { */
      if chunksize(T) ^= S then do;
        /* tchunkptr* C = &(T->child[(K >> (SIZE_T_BITSIZE-SIZE_T_ONE)) & 1]) */
	/* Select child(0) or child(1) depending on whether */
	/* the MSB of the current chunksize is even or odd  */
        C = addr(T->child(IAND(ISRL(K,(31)),1)));
        /* Shift chunksize to look at next bit */
        K = ISLL(k,1);
        if C->aPtr ^= sysnull()
        then T = c->aPtr;
        else do;                        /* empty leaf node */
          c->aPtr = x;
          x->malloc_tree_chunk.parent = t;
          x->malloc_tree_chunk.fd,x->malloc_tree_chunk.bk = X;
          leave for; /* break */
          end; /* else */
        end; /* chunksize */
      else do;                          /* duplicate size chunk */
        F = T->malloc_tree_chunk.fd;
        T->malloc_tree_chunk.fd, F->malloc_tree_chunk.bk = X;
        x->malloc_tree_chunk.fd = F;
        x->malloc_tree_chunk.bk = T;
        x->malloc_tree_chunk.parent = sysnull();
        leave for; /* break */
	end; /* else */
      end; /* for */
    end; /* else */

  end insert_large_chunk;
 
/* Unlink a large chunk */
/* #define unlink_large_chunk(M, X) {\
 *  tchunkptr XP = X->parent;\
 *  tchunkptr R;\
 *  if (X->bk != X) {\
 *    tchunkptr F = X->fd;\
 *    R = X->bk;\
 *    if (RTCHECK(ok_address(M, F))) {\
 *      F->bk = R;\
 *      R->fd = F;\
 *    }\
 *    else {\
 *      CORRUPTION_ERROR_ACTION(M);\
 *    }\
 *  }\
 *  else {\
 *    tchunkptr* RP;\
 *    if (((R = *(RP = &(X->child[1]))) != 0) ||\
 *        ((R = *(RP = &(X->child[0]))) != 0)) {\
 *      tchunkptr* CP;\
 *      while ((*(CP = &(R->child[1])) != 0) ||\
 *             (*(CP = &(R->child[0])) != 0)) {\
 *        R = *(RP = CP);\
 *      }\
 *      if (RTCHECK(ok_address(M, RP)))\
 *        *RP = 0;\
 *      else {\
 *        CORRUPTION_ERROR_ACTION(M);\
 *      }\
 *    }\
 *  }\
 *  if (XP != 0) {\
 *    tbinptr* H = treebin_at(M, X->index);\
 *    if (X == *H) {\
 *      if ((*H = R) == 0) \
 *        clear_treemap(M, X->index);\
 *    }\
 *    else if (RTCHECK(ok_address(M, XP))) {\
 *      if (XP->child[0] == X) \
 *        XP->child[0] = R;\
 *      else \
 *        XP->child[1] = R;\
 *    }\
 *    else\
 *      CORRUPTION_ERROR_ACTION(M);\
 *    if (R != 0) {\
 *      if (RTCHECK(ok_address(M, R))) {\
 *        tchunkptr C0, C1;\
 *        R->parent = XP;\
 *        if ((C0 = X->child[0]) != 0) {\
 *          if (RTCHECK(ok_address(M, C0))) {\
 *           R->child[0] = C0;\
 *            C0->parent = R;\
 *          }\
 *          else\
 *            CORRUPTION_ERROR_ACTION(M);\
 *        }\
 *        if ((C1 = X->child[1]) != 0) {\
 *          if (RTCHECK(ok_address(M, C1))) {\
 *            R->child[1] = C1;\
 *            C1->parent = R;\
 *          }\
 *         else\
 *            CORRUPTION_ERROR_ACTION(M);\
 *        }\
 *      }\
 *      else\
 *        CORRUPTION_ERROR_ACTION(M);\
 *    }\
 *  }\
 *}
 */
unlink_large_chunk: proc(x);
  dcl    x                   ptr;    /* tchunkptr */
  dcl    xp                  ptr;    /* tchunkptr */
  dcl    R                   ptr;    /* tchunkptr */
  dcl    F                   ptr;    /* tchunkptr */
  dcl   (CP,RP)              ptr;    /* tchunkptr* */
  dcl   (C0,C1)              ptr;    /* tchunkptr* */
  dcl    H                   ptr;    /* tbinptr* */

  xp = x->malloc_tree_chunk.parent;
  /* If this chunk is not the first for its size */
  /* just unlink it                              */	 
  if x->malloc_chunk.bk ^= x then do;  /* Not first on chain */
    F = X->malloc_chunk.fd;
    R = X->malloc_chunk.bk;
    F->malloc_chunk.bk = R;
    R->malloc_chunk.fd = F;
    end;
  else stuff:do;
    /* Danger Will Robinson!  The original C code depends on         */
    /* short-circuiting the evaluation of the boolean expression     */
    /* and hence the assignment to R once a TRUE condition is        */
    /* encountered.  PL/I can't be guaranteed to work this way, and  */
    /* in fact is defined to evaluate the entire expression,         */
    /* although in practice most compilers probably short-circuit.   */
    /* tchunkptr* RP;\
     * if (((R = *(RP = &(X->child[1]))) != 0) ||\
     *     ((R = *(RP = &(X->child[0]))) != 0)) {\
     *   tchunkptr* CP;\
     *   while ((*(CP = &(R->child[1])) != 0) ||\
     *          (*(CP = &(R->child[0])) != 0)) {\
     *     R = *(RP = CP);\
     *   }\
     *   if (RTCHECK(ok_address(M, RP)))\
     *     *RP = 0;\
     *   else {\
     *     CORRUPTION_ERROR_ACTION(M);\
     *   }\
     * }\
     *}\
     */
    /* If this chunk is the first or only one for its size */
    /* it needs to be removed from its parent/child        */
    /* relationship                                        */
    if assign2(R,RP,addr(X->malloc_tree_chunk.child(1))) ^= sysnull() |
       assign2(R,RP,addr(X->malloc_tree_chunk.child(0))) ^= sysnull()
    then do;
      /* Chunk has children */
      do while( assign1(cp,addr(R->malloc_tree_chunk.child(1))) ^=sysnull() |
                assign1(cp,addr(R->malloc_tree_chunk.child(0))) ^=sysnull()
	      );
        r = assign1(rp,cp);                   /* Search from leftmost child */
        end; /* do while */
      if rp>=malloc_state.least_addr          /* ok_address(M, RP) */
      then rp->aPtr = sysnull;                /* *rp=0 */
      else do;                                                /*0.9.3*/
        call release_lock( addr(malloc_state.mutex) );        /*0.9.3*/
        signal error;
	end;                                                  /*0.9.3*/
      end; /* then */
      end stuff; /* then */
 
  if XP ^= sysnull() then do;
    /* H = treebin_at(M, X->index); */
    H = addr(malloc_state.treebins(X->malloc_tree_chunk.index));  
    /* If this chunk has a parent (XP), it is the first */
    /* chunk of this size, if it's only, just clear treemap */
    if X = H->aPtr then do;          /* if (X==*H) */
      H->aPtr = R;                   /* if ((*H=R)==0) */
      if H->aPtr = sysnull() then do;
        /* clear_treemap(M, X->malloc_tree_chunk.index) */
        /* #define clear_treemap(M,i) ((M)->treemap  &= ~idx2bit(i)) */
        malloc_state.treemap = IAND(malloc_state.treemap, 
          INOT(ISLL(1,X->malloc_tree_chunk.index)));
	end;
      end;
    else do;
      /* If this is not the first chunk of this size, unchain it */
      if xp>=malloc_state.least_addr then do;
        if x = xp->malloc_tree_chunk.child(0)
        then xp->malloc_tree_chunk.child(0) = R;
        else xp->malloc_tree_chunk.child(1) = R;
	end;
      end; /* else */
    if R ^= sysnull() then do;
      if r>=malloc_state.least_addr then do;   
        R->malloc_tree_chunk.parent = XP;
        C0 = X->malloc_tree_chunk.child(0);
        if C0 ^=sysnull() & C0>=malloc_state.least_addr then do;
          R->malloc_tree_chunk.child(0) = C0;
          C0->malloc_tree_chunk.parent = R;
          end;
        C1 = X->malloc_tree_chunk.child(1);
        if C1 ^=sysnull() & C1>=malloc_state.least_addr then do;
          R->malloc_tree_chunk.child(1) = C1;
          C1->malloc_tree_chunk.parent = R;
          end;
	end;
      else do;                                                /*0.9.3*/
        call release_lock( addr(malloc_state.mutex) );        /*0.9.3*/
        signal error;	  
	end;                                                  /*0.9.3*/
      end; /* R */
    end; /* XP */
 
  /* assign1 and assign2 are used to replace C's assignments in        */
  /* boolean expressions.                                              */
  /* assign c to b, *b to a, and return a */
  assign2: proc(a,b,c) returns( ptr );
    dcl   (a,b,c)            ptr;
    b = c;
    a=b->aPtr; /* *b */
    return(a);
    end assign2;
    
  /* assign b to a, and return *a         */
  assign1: proc(a,b) returns( ptr );
    dcl   (a,b)              ptr;
    a = b;
    return(a->aPtr); /* *a */
    end assign1;

  end unlink_large_chunk;

/* Link a free chunk into a smallbin  */
/* #define insert_small_chunk(M, P, S) {\
 *   bindex_t I  = small_index(S);\
 *   mchunkptr B = smallbin_at(M, I);\
 *   mchunkptr F = B;\
 *   assert(S >= MIN_CHUNK_SIZE);\
 *   if (!smallmap_is_marked(M, I))\
 *     mark_smallmap(M, I);\
 *   else if (RTCHECK(ok_address(M, B->fd)))\
 *     F = B->fd;\
 *   else {\
 *     CORRUPTION_ERROR_ACTION(M);\
 *   }\
 *   B->fd = P;\
 *   F->bk = P;\
 *   P->fd = F;\
 *   P->bk = B;\
 * }
 */
insert_small_chunk: proc(P, S);
  dcl     P                   ptr;
  dcl     S                   fixed bin(31);
  dcl     I                   fixed bin(31);      /* bindex_t */
  dcl    (B,F)                ptr;                /* mchunkptr */

  I =ISRL(S,SMALLBIN_SHIFT);     /* small_index(S) */
  /* #define smallbin_at(M, i) ((sbinptr)((char*)&((M)->smallbins[(i)<<1]))) */
  B = addr(malloc_state.smallbins(ISLL(I,1)));   /* smallbin_at(M, I) */
  F = B;
  /* #define smallmap_is_marked(M,i) ((M)->smallmap &   idx2bit(i)) */
  /* #define mark_smallmap(M,i) ((M)->smallmap |=  idx2bit(i)) */
  /* #define idx2bit(i)              ((binmap_t)(1) << (i))    */
  if ^(IAND(malloc_state.smallmap,ISLL(1,I)))   /* ^smallmap_is_marked(M, I) */
  then malloc_state.smallmap = IOR(malloc_state.smallmap,ISLL(1,i));   /* mark_smallmap(M, I) */
  else if B->malloc_chunk.fd>=malloc_state.least_addr
  then F = B->malloc_chunk.fd;
  else do;                                                    /*0.9.3*/
    call release_lock( addr(malloc_state.mutex) );            /*0.9.3*/
    signal error;
    end;                                                      /*0.9.3*/
  B->malloc_chunk.fd = P;
  F->malloc_chunk.bk = P;
  P->malloc_chunk.fd = F;
  P->malloc_chunk.bk = B;
  end insert_small_chunk;

/* Unlink a chunk from a smallbin  */
/* #define unlink_small_chunk(M, P, S) {\
 *   mchunkptr F = P->fd;\
 *   mchunkptr B = P->bk;\
 *   bindex_t I = small_index(S);\
 *   assert(P != B);\
 *   assert(P != F);\
 *   assert(chunksize(P) == small_index2size(I));\
 *   if (F == B)\
 *     clear_smallmap(M, I);\
 *   else if (RTCHECK((F == smallbin_at(M,I) || ok_address(M, F)) &&\
 *                    (B == smallbin_at(M,I) || ok_address(M, B)))) {\
 *     F->bk = B;\
 *     B->fd = F;\
 *   }\
 *   else {\
 *     CORRUPTION_ERROR_ACTION(M);\
 *   }\
 * }
 */
unlink_small_chunk: proc(P, S);
  dcl    P                   ptr;                /* mchunkptr */
  dcl    S                   fixed bin(31);      /* size_t */
  dcl   (F,B)                ptr;                /* mchunkptr */
  dcl    I                   fixed bin(31);      /* bindex_t */
  dcl    j                   fixed bin(31);      /* PL/I */
  F = P->malloc_chunk.fd;
  B = P->malloc_chunk.bk;
  i = ISRL(s,SMALLBIN_SHIFT);    /* small_index(S) */
  j = ISLL(i,1);
  if F = B                       /* clear_smallmap(M, I) */
  then malloc_state.smallmap = IAND(malloc_state.smallmap,INOT(ISLL(1,i)));
  else do;
    /* #define smallbin_at(M, i) ((sbinptr)((char*)&((M)->smallbins[(i)<<1]))) */
    if (f = addr(malloc_state.smallbins(j)) | f>=malloc_state.least_addr) &                
       (b = addr(malloc_state.smallbins(j)) | b>=malloc_state.least_addr) 
    then do;                
      F->malloc_chunk.bk = B;
      B->malloc_chunk.fd = F;
      end;
    else do;
      call release_lock( addr(malloc_state.mutex) );          /*0.9.3*/
      signal error;
      end;
    end;
  end unlink_small_chunk;

/* Check some properties of malloc_state. */
do_check_malloc: proc();
  dcl    i                   fixed bin(31); /* bindex_t */
  dcl    total               fixed bin(31); /* size_t */
  total = traverse_and_check();
  end do_check_malloc;
  
/* Traverse each chunk and check it; return total */
traverse_and_check: proc() returns( fixed bin(31) );
  dcl    sum                 fixed bin(31)  init(0);
  dcl    s                   ptr;                /* msegmentptr */
  dcl    q                   ptr;                /* mchunkptr */
  dcl    lastq               ptr;                /* mchunkptr */
  
  if malloc_state.top^=sysnull() then do; /* is_initialized(m) */
    s = addr(malloc_state.seg);
    sum = sum +malloc_state.topsize + top_foot_size;
    do while(s ^= sysnull());
      /* q = align_as_chunk(s->malloc_segment.base); */
      /* #define align_as_chunk(A) (mchunkptr)((A) + align_offset(chunk2mem(A))) */
      q = s->malloc_segment.base + align_offset(s->malloc_segment.base + 8);
      lastq = sysnull();
      do while( q>=s->malloc_segment.base & 
                q<s->malloc_segment.base+s->malloc_segment.size &      /* segment_holds(s, q) */
                q ^= malloc_state.top & 
                q->malloc_chunk.head ^= FENCEPOST_HEAD);
        sum = sum + chunksize(q);
        /*
         * if cinuse(q)
         * then call do_check_inuse_chunk(m, q);
         * else call do_check_free_chunk(m, q);
         */
        lastq = q;
        /* #define next_chunk(p) ((mchunkptr)( ((char*)(p)) + ((p)->head & ~FLAG_BITS))) */
	q = q + IAND(q->malloc_chunk.head,INOT(FLAG_BITS)); /* next_chunk(q) */
        end;
      s = s->malloc_segment.next;
      end;
    end; /* is initialized */
  return( sum );
  end traverse_and_check;

/* the number of bytes to offset an address to align it */
/* #define align_offset(A)\
 *  ((((size_t)(A) & CHUNK_ALIGN_MASK) == 0)? 0 :\
 *   ((MALLOC_ALIGNMENT - ((size_t)(A) & CHUNK_ALIGN_MASK)) & CHUNK_ALIGN_MASK))
 */
align_offset: proc(a) returns( fixed bin(31) );
  dcl    a                   ptr;                /* char* */
  dcl    temp                fixed bin(31);
  if IAND(binvalue(a),CHUNK_ALIGN_MASK)^=0
  then return(0);
  temp = MALLOC_ALIGNMENT - IAND(binvalue(a),CHUNK_ALIGN_MASK);
  temp = IAND(temp,CHUNK_ALIGN_MASK);
  return(temp);
  end align_offset;
  
preaction: proc() returns( fixed bin(31) );
  /* Ensure locks are initialized */
  /* #define GLOBALLY_INITIALIZE() (mparams.page_size == 0 && init_mparams()) */
  /* #define PREACTION(M)  ((GLOBALLY_INITIALIZE() || use_lock(M))? ACQUIRE_LOCK(&(M)->mutex) : 0) */
  if (mparams.page_size=0 & init_mparams^=0) | IAND(malloc_state.mflags,USE_LOCK_BIT)
  then call acquire_lock( addr(malloc_state.mutex) );
  return(0);
  end preaction;

/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/
/* Replacements for Microsoft _BitScanForward and _BitScanReverse     */
/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/
BitScanForward: proc(index,word) returns( fixed bin(31) );
  dcl    word                fixed bin(31);
  dcl    index               fixed bin(31);
  dcl    tmp                 bit(32);
  dcl    i                   fixed bin(31);
  dcl    fbx                 fixed bin(31) based;
  addr(tmp)->fbx = bsw(word);                      /* Big-endian     */
  do i=32 to 1 by -1;
    if substr(tmp,i,1) then do;
      index = 32-i;
      return(1);                       /* Bit found      */
      end;
    end; /* do i */
  index=0;
  return(0);                           /* No bit found   */
  end BitScanForward;

BitScanReverse: proc(index,word) returns( fixed bin(31) );
  dcl    word                fixed bin(31);
  dcl    Bits                bit(32) based;
  dcl    index               fixed bin(31);
  dcl    tmp                 bit(32);
  dcl    i                   fixed bin(31);
  dcl    fbx                 fixed bin(31) based;
  addr(tmp)->fbx = bsw(word);                      /* Big-endian     */
  do i=1 to 32;
    if substr(tmp,i,1) then do;
      index = 32-i;
      return(1);                       /* Bit found      */
      end;
    end; /* do i */
  index=0;
  return(0);                           /* No bit found   */
  end BitScanReverse;

/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/
/* Replacements for Enterprise PL/I bit manipulation builtins        */
/* These versions require that the arguments be FIXED BIN(31,0)      */
/* and return a FIXED BIN(31,0) result.                              */
/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/

IAND: proc(f1,f2) returns( fixed bin(31) );
  dcl   (f1,f2)             fixed bin(31);
  dcl    bb                  bit(32) based;
  dcl    fbx                 fixed bin(31);
  addr(fbx)->bb = addr(f1)->bb & addr(f2)->bb;
  return( fbx );
  end IAND;

IOR: proc(f1,f2) returns( fixed bin(31) );
  dcl   (f1,f2)              fixed bin(31);
  dcl    bb                  bit(32) based;
  dcl    fbx                 fixed bin(31);
  addr(fbx)->bb = addr(f1)->bb | addr(f2)->bb;
  return( fbx );
  end IOR;

IEOR: proc(f1,f2) returns( fixed bin(31) );
  dcl   (f1,f2)              fixed bin(31);
  dcl    bb                  bit(32) based;
  dcl    fbx                 fixed bin(31);
  addr(fbx)->bb = bool( addr(f1)->bb, addr(f2)->bb, '0110'b );
  return( fbx );
  end IEOR;

INOT: proc(fb) returns( fixed bin(31) );
  dcl    fb                  fixed bin(31);
  dcl    bb                  bit(32) based;
  dcl    fbx                 fixed bin(31);
  fbx = fb;
  addr(fbx)->bb = ^addr(fbx)->bb;
  return( fbx );
  end INOT;

ISLL: proc(fb,n) returns( fixed bin(31) );
  dcl    fb                  fixed bin(31);
  dcl    n                   fixed bin(31);
  dcl    fbx                 fixed bin(31);
  dcl    bb                  bit(32) based;
  if n>=32 then return(0);
  if n<=0  then return(fb);
  fbx = BSW(fb);
  addr(fbx)->bb = substr(addr(fbx)->bb,n+1);
  fbx = BSW(fbx);
  return( fbx );
  end ISLL;

ISRL: proc(fb,n) returns( fixed bin(31) );
  dcl    fb                  fixed bin(31);
  dcl    n                   fixed bin(31);
  dcl   (fb1,fb2)            fixed bin(31);
  dcl    bb                  bit(32) based;
  if n>=32 then return(0);
  if n<=0  then return(fb);
  fb1 = BSW(fb);
  fb2 = 0;
  substr(addr(fb2)->bb,n+1) = addr(fb1)->bb;
  fb2 = BSW(fb2);   
/* 
  fb2 = fb / 2**n;
 */
  return( fb2 );
  end ISRL;

/* Swap bytes in a dword to conver FIXED BIN to BIT */ 
BSW: proc(fb) returns( fixed bin(31) );
  dcl    fb                  fixed bin(31);
  dcl    p                   ptr;
  dcl    cx               (4)char(1) based( addr(fb) );
  dcl    cc               (4)char(1);
  dcl    fbx                 fixed bin(31) based;
  p = addr(fb);
  cc(1) = cx(4); cc(2) = cx(3);
  cc(3) = cx(2); cc(4) = cx(1);
  return( addr(cc)->fbx );
  end BSW;

/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/
/* Various Linux-specific system calls                               */
/*~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~*/
(nofixedoverflow):                                            /*0.9.3*/
 morecore: proc(nb) returns(ptr);
   dcl   nb                  fixed bin(31);
   dcl   top                 fixed bin(31);
   dcl   RC                  fixed bin(31);
   top = syscall(SYS_BRK,0);         /* Get current top of .bss seg  */
   if nb=0 then return( ptrvalue(top) );                      /*0.9.3*/
   RC = syscall(SYS_BRK,top+nb);     /* Expand .bss segment          */
   if RC = -ENOMEM then do;                                   /*0.9.3*/
     call release_lock( addr(morecore_mutex) );               /*0.9.3*/
     call release_lock( addr(malloc_state.mutex) );           /*0.9.3*/
     signal STORAGE;                 /* Presumably no storage avail. */
     end;                                                     /*0.9.3*/
   return( ptrvalue(top) );
   end morecore;

/* Replacemment for C time(0) function */
c_time: proc(n) returns( fixed bin(31) );
   dcl   n                   fixed bin(31);
   dcl   tod                 fixed bin(31);
   tod = syscall(SYS_TIME,0);         /* Get current time */
   return(tod);
   end c_time;
   
release_lock: proc(pMutex);
  dcl     pMutex              ptr;
  dcl     RC                  fixed bin(31);
  call barrier;                       /* Memory fence           0.9.2*/
  RC = mutex_post( pMutex );                                  /*0.9.2*/
  if RC<0 then do;
    signal error;
    end;
  end release_lock;
   
acquire_lock: proc(pMutex);
  dcl     pMutex              ptr;
  dcl     RC                  fixed bin(31);
again:  
  RC = mutex_wait( pMutex );                                  /*0.9.2*/
  if RC=-11 then goto again;
  if RC<0 then do;
    /* What to do if a wait is interrupted? */
    signal error;
    end;
  call barrier;                         /* Memory fence         0.9.2*/
  end acquire_lock;
  
  
dump_large: proc;
  dcl i fixed bin(31);
  display( 'Tree Dump' );  
  do i=0 to NTREEBINS-1;
    if malloc_state.treebins(i)^=sysnull()
    then call dump_bin( malloc_state.treebins(i) );
    end; /* do */
    
dump_bin: proc(b);
  dcl b ptr;
  dcl p ptr;
  p = b;
  do while('1'b);
    call dump_bin1(p,0);
    p = p->malloc_tree_chunk.fd;
    if p=b then leave;
    end; /* do while */
  end dump_bin;

dump_bin1: proc(p,n);
  dcl p ptr;
  dcl n fixed bin(7);
  dcl spaces char(8) static init( '' );
  display( substr(spaces,1,n)  ||
           heximage(addr(p),4) || p->malloc_tree_chunk.head || 
           p->malloc_tree_chunk.index || ' ' ||
           heximage(addr(p->malloc_tree_chunk.parent),4)   || ' ' ||
           heximage(addr(p->malloc_tree_chunk.child(0)),4) || ' ' ||
           heximage(addr(p->malloc_tree_chunk.child(1)),4) );
  if p->malloc_tree_chunk.child(0)^=sysnull
  then call dump_bin1(p->malloc_tree_chunk.child(0),4);	   
  if p->malloc_tree_chunk.child(1)^=sysnull
  then call dump_bin1(p->malloc_tree_chunk.child(1),4);	   
  end dump_bin1;    
    
  end dump_large;  
  
dump_small: proc();
  dcl b ptr;
  dcl i fixed bin(31);
  dcl p ptr;
  display( 'dump small bins' );
  b = addr(malloc_state.smallbins(0));
  do i=0 to NSMALLBINS-1;
    if ^(IAND(malloc_state.smallmap,ISLL(1,I)))   /* ^smallmap_is_marked(M, I) */
    then display( 'bin' || i || ' marked empty' );
    else do;
      display( 'bin' || i || ' at ' || heximage(addr(b),4) || ':' );
      p = b->malloc_chunk.fd;
      do while('1'b);
        if p = b then leave;
        display( '  ' || heximage(addr(p),4) || p->malloc_chunk.head ||
	         ' (' || heximage(p,16) || ')' );
	if p = p->malloc_chunk.fd then leave;
        p = p->malloc_chunk.fd;
        end; /* do while */    
      end;
    b = b+8;
    end; /* do i */
  end dump_small;  
  
  error_dump: proc;
    call dump_small;
    call dump_large;
    end error_dump;
  
  end heap_mgr;
